{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import collections\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299671 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.10\n",
      "================================================================================\n",
      "n shi  oehzspmoar sptsk erxhhqe    erjppsmeisyz zitw mcvh ahy daidhttpbyj u lzoe\n",
      "kcrnd ycwop uymgrkdwuncd ehn eurpp  ndhbniyt  l faqrkgeenciwvwd oyos nbpsrejmjf \n",
      "tt ein tppuernb lgqqr p tsoydgnyarielv wso eewjijedqay   gusoxk ggcyt crlrjacswe\n",
      "xpgjjcgsooznb yxlscsdkbkoeim dm rin ao at wtxcrmknk a ilecgkcmctripdhorcer tpwex\n",
      "gpdu  m fgmr biwoihcvgih kilinsymxoai onmiwerw ueyoe ctiync ehctatfmmbdfvruvsfar\n",
      "================================================================================\n",
      "Validation set perplexity: 20.28\n",
      "Average loss at step 100: 2.595023 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.13\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 200: 2.255872 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 300: 2.101554 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.47\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 2.003385 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.936827 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.906951 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 700: 1.859533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.70\n",
      "Average loss at step 800: 1.817758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.94\n",
      "Validation set perplexity: 6.43\n",
      "Average loss at step 900: 1.828674 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.826420 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "ch fred will gov dipleativies maso a bene desectev on of allowing he inferen and\n",
      "ble firn his rowrations digs elviolvy mey floage into from one one nive one lile\n",
      "ial arges chenich moters compite it fics sever one by couth of the misce skill c\n",
      "s evain boor hay uremanis while lepiting unding lesjecen orwaly specized as encu\n",
      "come the to spria ling soal in teroin one five zerorns in moxil reality s gead o\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.773264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.90\n",
      "Average loss at step 1200: 1.753164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1300: 1.734155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1400: 1.746489 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1500: 1.734505 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 5.58\n",
      "Average loss at step 1600: 1.743875 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1700: 1.711568 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1800: 1.672523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1900: 1.646098 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.694989 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "s and gemition a bnestem two and eumony be was graph hests and wotle thoughn tor\n",
      "med comed bilmodly dissixed produced be one nine eight six three eight to six fi\n",
      "pablin on a pach gat dvollones who anyagera that charles to resefved that wendin\n",
      "finational it knjinsias lina soupice of progrisment cersited that is ba world fa\n",
      "dombed thannion is andied proul the semnic is ontwaiding examaes b this frieinal\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.686591 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.677744 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 2300: 1.642461 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 2400: 1.660110 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500: 1.682542 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2600: 1.649877 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.76\n",
      "Average loss at step 2700: 1.656159 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.647322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 2900: 1.649263 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 3000: 1.644727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "chers ofcipion duc one marinioy woyloving and eargarter of live and their and tw\n",
      "ine depover incleanly waltenuh then dow demplicard advanote scoincers sciplier w\n",
      "quilationias laing of inveryworism amagced five sears ployer and highy of antyly\n",
      " theary foll of kindon are creatrent the barline passin simpose was in the ofriv\n",
      "dome yuplentistitieved that toanson of blirf unco necled jassa westionspand graw\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3100: 1.621806 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3200: 1.641268 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3300: 1.633429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3400: 1.664172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3500: 1.652964 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3600: 1.665806 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 3700: 1.639768 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3800: 1.639709 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.633865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4000: 1.647420 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "================================================================================\n",
      "ged sle propips def depitied goint on the world domet latagoure free site inwara\n",
      "s depeal mupporatal new herg in the rbaina other uses the held cuber pas i arusl\n",
      "ency acceatrortces or gell mathice masit coance maney offiliticd bypici among iv\n",
      "l end these line theysitor title the he one after on the equlatity or abruccity \n",
      "va canive in becenter in one five zero zero zero zero be emparn and ut bemicid a\n",
      "================================================================================\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4100: 1.629296 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4200: 1.633337 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 4300: 1.611490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4400: 1.609714 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.613439 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4600: 1.613343 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.624093 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4800: 1.629429 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4900: 1.630399 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5000: 1.607281 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.43\n",
      "================================================================================\n",
      "jonaime one nine four englandter east and would aavilyumatize but often to majic\n",
      "a jusine one eight four rubols of be is tradel not death sha non and or idlanded\n",
      "iv the miths by pablese called am mination vahne interruftier of the topational \n",
      "ing may and trans being dispedlic sydy elivinated calq proor bee conty each is d\n",
      "var is hendsan ten with goppeneses indemaise mother bunised the lavies in outs e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5100: 1.604175 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.590933 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5300: 1.576912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 5400: 1.573808 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5500: 1.561611 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.580291 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5700: 1.564998 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.37\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5800: 1.578688 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.579179 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6000: 1.547217 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "ous betai according port abs a propu so would one nine two one six various on on\n",
      "x riduations funs malevent thot mocame as ragges on that for the hest agring tha\n",
      "vil themicrade west metord reast incleatest metime bujd are of demass is its sit\n",
      "uss cuter was the but the teamed the such and heragbol this years forch suplocip\n",
      "ew hank in one change a wattent which island meet upuded theois manish becase in\n",
      "================================================================================\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6100: 1.564346 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6200: 1.535467 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6300: 1.544311 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6400: 1.538784 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6500: 1.556759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.594040 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6700: 1.578714 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.22\n",
      "Average loss at step 6800: 1.600830 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6900: 1.577653 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 7000: 1.572881 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "zer p acthors that mpicted dincopes danumation with the fories that io juagery p\n",
      "creating its has in there eccevale were ambthere his sims read words celeage of \n",
      "vers battes be power sider important repriptible ecaininative to a coperians sac\n",
      "y under to all with odupuate of the sudebel in the concesson out castremenclas w\n",
      "mon the utulism consastich the s an quaket heady ferrheltony severation a lif pr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    \n",
    "  W_i = tf.concat([ix, fx, cx, ox], 1)\n",
    "  W_h = tf.concat([im, fm, cm, om], 1)\n",
    "  b_i = tf.concat([ib, fb, cb, ob], 1)\n",
    "    \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    cell = tf.matmul(i, W_i) + tf.matmul(o, W_h) + b_i\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(cell, 4, axis = 1)\n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 27) for Tensor u'Placeholder_8:0', which has shape '(128, 27)'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ae0c1a3df727>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     _, l, predictions, lr = session.run(\n\u001b[0;32m---> 14\u001b[0;31m       [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m                 \u001b[0;34m'which has shape %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m                 % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n\u001b[0m\u001b[1;32m    945\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Tensor %s may not be fed.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot feed value of shape (64, 27) for Tensor u'Placeholder_8:0', which has shape '(128, 27)'"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 64)\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # initializing only one matrix\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  W_i = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "  W_h = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  b_i = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    cell = tf.matmul(i, W_i) + tf.matmul(o, W_h) + b_i\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(cell, 4, axis = 1)\n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32,\n",
    "                                     shape=[batch_size, vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    print(tf.concat(outputs, 0).shape)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294532 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "nmbtyvp olho  q aadety enlrutaxruwhsevrrz eteawec  bnleqciittrtqebpl neee hxw r \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "gdemyudnkbowohf cjh om teams  n au  cquep vnowqvrexrqvu avziuutis re  ezerw uan \n",
      "[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "cci ya htmyseygnne hu  n vi acn owr dnpbserxomtisp xesewb wpjkrinwea  evcauewno \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "tni  saqjg vr m dct zerjhtlfvjfxznazw anmuincmnkbarsenait cbitn vledsfpynnektsof\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "rveekueaie esxno  i tjza vcsefxiteuesbqlntzeknsbppk etgenewrb jbeqeyvtdpfeitdreo\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 100: 2.579778 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.22\n",
      "Validation set perplexity: 10.54\n",
      "Average loss at step 200: 2.289772 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.81\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 300: 2.132264 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.47\n",
      "Validation set perplexity: 8.43\n",
      "Average loss at step 400: 2.044024 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.78\n",
      "Validation set perplexity: 7.81\n",
      "Average loss at step 500: 1.950909 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.73\n",
      "Validation set perplexity: 7.48\n",
      "Average loss at step 600: 1.946937 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.19\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.916409 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.76\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 800: 1.859245 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 900: 1.845267 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 6.35\n",
      "Average loss at step 1000: 1.821711 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "man eares freex beer contu und iarouley to of cersominters of head frc one suide\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "nce rejule meby an to six fine sister uss ochinish the browed to the modus s the\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "o at trumpty v boncentens lother ingle five on seres native wime nine zero jasch\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "n pasects zeres a fimmero coirtroups so mott perija beht whith have court fistro\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "mebi lase ilfamyle hos a unitions to coutureat of it and lole day vielos canam m\n",
      "================================================================================\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1100: 1.811941 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.97\n",
      "Validation set perplexity: 6.17\n",
      "Average loss at step 1200: 1.787068 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1300: 1.778925 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1400: 1.763032 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1500: 1.734769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1600: 1.732099 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.74\n",
      "Average loss at step 1700: 1.699601 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1800: 1.692732 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 1900: 1.683832 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 2000: 1.700155 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "en the towish a raye mangarcy after shoux buncies siffernes a suing by miblure a\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "ing pisportingle two zero milli s deloter is their s preet in sakgue conbrustrin\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "p combebution with area binglomop a conope knowle white man their with by one vi\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "were forging other fine and of planizoth meonst goldrar pandal they conthons car\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "lign alter four e presien acsossubsed the opher on one nine five whod charcolly \n",
      "================================================================================\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 2100: 1.721352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.60\n",
      "Average loss at step 2200: 1.714356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 2300: 1.714426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2400: 1.684944 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2500: 1.693307 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2600: 1.676890 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2700: 1.673321 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2800: 1.663797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2900: 1.688524 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 3000: 1.679928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "fered in a bill and the comside europent that the manimation the ewide indold th\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "s bacy now to a war be linefbal were dayal laubra ahelace of tapthing precins wy\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "raye unditionary tric havan be pairlian calan envidots vilciolation is of sourch\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "hos hel is a gother to defflessed forke in twyper berovestill muth luse the ligh\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "n dereight a numeen parchia may and hucht ies ub magm five ziradual deficices ar\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.633949 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 3200: 1.640232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3300: 1.671955 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3400: 1.668715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 3500: 1.698554 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3600: 1.661667 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 3700: 1.647522 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3800: 1.638517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 3900: 1.654224 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4000: 1.642584 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "diel of oxigatured mother for d covere dogus the nine the blish elond to confire\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "que ind f is one nine the leupessims of hadue beliech be generally placer the ne\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "ver collegews rather destemit zercen russ of the was heak in bit werear patther \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "veric obcherphato is both avouting j the septur the dy overmenss conse one nitro\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "que enginess a defits one two contract bruthongecation the decause infell uncomp\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4100: 1.649126 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4200: 1.631558 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.26\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 4300: 1.610891 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4400: 1.628238 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4500: 1.615689 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 4600: 1.629487 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4700: 1.653672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 4800: 1.654012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4900: 1.633913 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 5000: 1.652439 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  1.  0.  0.  0.  0.  0.  0.]]\n",
      "te lowinely for some follway of the the ebrainers corf can a s canny as and amer\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "que the onranger mallen houth and kada monitic proves the the war to that lith d\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  1.  0.  0.  0.  0.]]\n",
      "ver the suckor and detay intrupled the from the homing the prestruer originally \n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "reso shtare phy aroby flucines ecansiy crudfal counter michounced is found one t\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "x but a procise in the e d the succisit of wild brossochanime he can bruaz than \n",
      "================================================================================\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 5100: 1.625682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 5200: 1.633654 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 5300: 1.633187 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 5400: 1.634318 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 5500: 1.645669 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5600: 1.612235 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 5700: 1.606713 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 5800: 1.613362 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5900: 1.613787 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6000: 1.647226 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.51\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "grestary for simpny s wales reblecians the use poe volounetary b comon pidfole o\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n",
      "wore theylestant as the culersm by many estudiest directions typures some of may\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "jent six be is distribuar quest hill round stites meorish four everyer in ofcrop\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "moldow for missicula in the one nicheray designer in that her porks john fould f\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "m the follown as depatiuty the farmetispics and throg dislack the stocrage to th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 6100: 1.623904 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6200: 1.635649 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6300: 1.637604 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 6400: 1.635174 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 6500: 1.628781 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6600: 1.605878 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6700: 1.608929 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6800: 1.614311 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 6900: 1.603914 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 7000: 1.630042 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "henidine to gabonay h from churson souncedom in one zero zero two lang for the f\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "las in son deportanch blods left the gonest amaggers country monal for leace or \n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "bian four multiclan multip compose stainance againalian and lighta vaval five cy\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "jed is lave afterh three limate shotuted beover mughting best itia one nine seve\n",
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
      "   0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "nory point landant denatessad repoliant one manged external between yuen beason \n",
      "================================================================================\n",
      "Validation set perplexity: 4.73\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "4 matrix initialization performed better than only one. View the above models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Bigrams: 728\n"
     ]
    }
   ],
   "source": [
    "def unique_bigrams(text):\n",
    "    all_bigrams = []\n",
    "    for i in range(0, len(text), 2):\n",
    "        bigram = text[i:i + 2]\n",
    "        all_bigrams.append(bigram)\n",
    "        \n",
    "    return np.unique(all_bigrams)\n",
    "\n",
    "bigrams = unique_bigrams(train_text)\n",
    "n_bigrams = len(bigrams)\n",
    "print('N Bigrams: {}'.format(n_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  anarchism originated as a term of abuse first use\n",
      "  >batch:  ['na', 'na', 'rc', 'rc', 'hi', 'hi', 'sm', 'sm', ' o', ' o', 'ri', 'ri', 'gi', 'gi', 'na', 'na', 'te', 'te', 'd ', 'd ', 'as', 'as', ' a', ' a', ' t', ' t', 'er', 'er', 'm ', 'm ', 'of', 'of', ' a', ' a', 'bu', 'bu', 'se', 'se', ' f', ' f', 'ir', 'ir', 'st', 'st', ' u', ' u', 'se', 'se', 'd ', 'd ', 'ag', 'ag', 'ai', 'ai', 'ns', 'ns', 't ', 't ', 'ea', 'ea', 'rl', 'rl', 'y ', 'y ', 'wo', 'wo', 'rk', 'rk', 'in', 'in', 'g ', 'g ', 'cl', 'cl', 'as', 'as', 's ', 's ', 'ra', 'ra', 'di', 'di', 'ca', 'ca', 'ls', 'ls', ' i', ' i', 'nc', 'nc', 'lu', 'lu', 'di', 'di', 'ng', 'ng', ' t', ' t', 'he', 'he', ' d', ' d', 'ig', 'ig', 'ge', 'ge', 'rs', 'rs', ' o', ' o', 'f ', 'f ', 'th', 'th', 'e ', 'e ', 'en', 'en', 'gl', 'gl', 'is', 'is', 'h ', 'h ', 're', 're', 'vo', 'vo']\n",
      "  >labels:  [' a', 'rc', 'na', 'hi', 'sm', 'rc', ' o', 'hi', 'sm', 'ri', 'gi', ' o', 'ri', 'na', 'te', 'gi', 'd ', 'na', 'as', 'te', 'd ', ' a', ' t', 'as', ' a', 'er', ' t', 'm ', 'of', 'er', ' a', 'm ', 'bu', 'of', 'se', ' a', 'bu', ' f', 'se', 'ir', ' f', 'st', ' u', 'ir', 'st', 'se', ' u', 'd ', 'se', 'ag', 'ai', 'd ', 'ag', 'ns', 'ai', 't ', 'ea', 'ns', 'rl', 't ', 'y ', 'ea', 'wo', 'rl', 'rk', 'y ', 'wo', 'in', 'rk', 'g ', 'in', 'cl', 'as', 'g ', 's ', 'cl', 'as', 'ra', 's ', 'di', 'ca', 'ra', 'ls', 'di', 'ca', ' i', 'nc', 'ls', 'lu', ' i', 'di', 'nc', 'ng', 'lu', ' t', 'di', 'he', 'ng', ' t', ' d', 'he', 'ig', 'ge', ' d', 'ig', 'rs', 'ge', ' o', 'f ', 'rs', 'th', ' o', 'e ', 'f ', 'en', 'th', 'e ', 'gl', 'is', 'en', 'h ', 'gl', 're', 'is', 'vo', 'h ', 'lu', 're']\n"
     ]
    }
   ],
   "source": [
    "class BigramBatchGenerator(object):\n",
    "  def __init__(self, text, bigrams, batch_size, skip_window, num_skips):\n",
    "    self._text = text\n",
    "    self._batch_size = batch_size\n",
    "    self._bigrams = bigrams\n",
    "    self._skip_window = skip_window\n",
    "    self._num_skips = num_skips\n",
    "    self._data_index = 0\n",
    "\n",
    "  def _next_bigram_id(self):\n",
    "    i = self._data_index\n",
    "    bigram = self._text[i:i + 2]\n",
    "    bigram_id = self._bigrams.index(bigram)\n",
    "    self._data_index = (i + 2) % len(self._text)\n",
    "    return bigram_id\n",
    "    \n",
    "  def next(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size), dtype=np.int32)\n",
    "    labels = np.zeros(shape=(self._batch_size, 1), dtype=np.int32)\n",
    "    \n",
    "    span = 2 * self._skip_window + 1 # [ skip_window target skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        bigram_id = self._next_bigram_id()\n",
    "        buffer.append(bigram_id)\n",
    "        \n",
    "    \n",
    "    for i in range(self._batch_size // self._num_skips):\n",
    "        target = self._skip_window  # target label at the center of the buffer\n",
    "        targets_to_avoid = [ self._skip_window ]\n",
    "        for j in range(self._num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * self._num_skips + j] = buffer[self._skip_window]\n",
    "            labels[i * self._num_skips + j, 0] = buffer[target]\n",
    "        bigram_id = self._next_bigram_id()\n",
    "        buffer.append(bigram_id)\n",
    "    \n",
    "    return (batch, labels)\n",
    "\n",
    "bigram_generator = BigramBatchGenerator(text, bigrams.tolist(), 128, 1, 2)\n",
    "(batch, labels) = bigram_generator.next()\n",
    "print('data:', text[:50])\n",
    "print('  >batch: ', [bigrams[i] for i in batch])\n",
    "print('  >labels: ', [bigrams[i] for i in labels.reshape(128)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "skip_window = 1 # How many words to consider left and right.\n",
    "num_skips = 2 # How many times to reuse an input to generate a label.\n",
    "# We pick a random validation set to sample nearest neighbors. here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent. \n",
    "valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "valid_window = 100 # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.array(random.sample(range(valid_window), valid_size))\n",
    "num_sampled = 64 # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default(), tf.device('/cpu:0'):\n",
    "\n",
    "  # Input data.\n",
    "  train_dataset = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "  \n",
    "  # Variables.\n",
    "  embeddings = tf.Variable(\n",
    "    tf.random_uniform([n_bigrams, embedding_size], -1.0, 1.0))\n",
    "  softmax_weights = tf.Variable(\n",
    "    tf.truncated_normal([n_bigrams, embedding_size],\n",
    "                         stddev=1.0 / math.sqrt(embedding_size)))\n",
    "  softmax_biases = tf.Variable(tf.zeros([n_bigrams]))\n",
    "  \n",
    "  # Model.\n",
    "  # Look up embeddings for inputs.\n",
    "  embed = tf.nn.embedding_lookup(embeddings, train_dataset)\n",
    "  # Compute the softmax loss, using a sample of the negative labels each time.\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.sampled_softmax_loss(weights=softmax_weights,\n",
    "                               biases=softmax_biases,\n",
    "                               inputs=embed,\n",
    "                               labels=train_labels,\n",
    "                               num_sampled=num_sampled,\n",
    "                               num_classes=n_bigrams))\n",
    "\n",
    "  # Optimizer.\n",
    "  # Note: The optimizer will optimize the softmax_weights AND the embeddings.\n",
    "  # This is because the embeddings are defined as a variable quantity and the\n",
    "  # optimizer's `minimize` method will by default modify all variable quantities \n",
    "  # that contribute to the tensor it is passed.\n",
    "  # See docs on `tf.train.Optimizer.minimize()` for more details.\n",
    "  optimizer = tf.train.AdagradOptimizer(1.0).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "Initialized\n",
      "Average loss at step 0: 4.292185\n",
      "Average loss at step 2000: 2.224269\n",
      "Average loss at step 4000: 1.988664\n",
      "Average loss at step 6000: 1.976471\n",
      "Average loss at step 8000: 1.966395\n",
      "Average loss at step 10000: 1.979750\n",
      "Average loss at step 12000: 1.965739\n",
      "Average loss at step 14000: 1.861095\n",
      "Average loss at step 16000: 2.007119\n",
      "Average loss at step 18000: 1.968653\n",
      "Average loss at step 20000: 1.957505\n",
      "Average loss at step 22000: 1.859731\n",
      "Average loss at step 24000: 1.956730\n",
      "Average loss at step 26000: 1.882391\n",
      "Average loss at step 28000: 1.902106\n",
      "Average loss at step 30000: 1.941708\n",
      "Average loss at step 32000: 1.898305\n",
      "Average loss at step 34000: 1.891825\n",
      "Average loss at step 36000: 1.922929\n",
      "Average loss at step 38000: 1.967797\n",
      "Average loss at step 40000: 1.955180\n",
      "Average loss at step 42000: 1.872121\n",
      "Average loss at step 44000: 1.963747\n",
      "Average loss at step 46000: 1.846275\n",
      "Average loss at step 48000: 1.923485\n",
      "Average loss at step 50000: 1.919927\n",
      "Average loss at step 52000: 1.906982\n",
      "Average loss at step 54000: 1.940540\n",
      "Average loss at step 56000: 1.897628\n",
      "Average loss at step 58000: 1.840963\n",
      "Average loss at step 60000: 1.888154\n",
      "Average loss at step 62000: 1.861616\n",
      "Average loss at step 64000: 1.869036\n",
      "Average loss at step 66000: 1.901339\n",
      "Average loss at step 68000: 1.958067\n",
      "Average loss at step 70000: 1.947671\n",
      "Average loss at step 72000: 1.879536\n",
      "Average loss at step 74000: 1.902167\n",
      "Average loss at step 76000: 1.922486\n",
      "Average loss at step 78000: 1.924458\n",
      "Average loss at step 80000: 1.928096\n",
      "Average loss at step 82000: 1.892321\n",
      "Average loss at step 84000: 1.898879\n",
      "Average loss at step 86000: 1.935602\n",
      "Average loss at step 88000: 1.902309\n",
      "Average loss at step 90000: 1.900643\n",
      "Average loss at step 92000: 1.974328\n",
      "Average loss at step 94000: 1.857070\n",
      "Average loss at step 96000: 1.835793\n",
      "Average loss at step 98000: 1.978257\n",
      "Average loss at step 100000: 1.856852\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "bigram_generator = BigramBatchGenerator(text, bigrams.tolist(), batch_size, \n",
    "                                        skip_window, num_skips)\n",
    "print('ok')\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    average_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        (batch_data, batch_labels) = bigram_generator.next()\n",
    "        feed_dict = {train_dataset : batch_data, train_labels : batch_labels}\n",
    "        (_, l) = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "        average_loss += l\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss = average_loss / 2000\n",
    "            # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step %d: %f' % (step, average_loss))\n",
    "    learned_embeddings = embeddings.eval()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[418 392 628  19 335 490   0 520 351 521 220  45 674   3  51 527 548  33\n",
      " 359  25 382 520 220 519 134 343 136  13 404 335 679 420 422 498  26   5\n",
      "  19  46  44 134 262 137   0 544 548 220 132 138 161 179  46 548 134 262\n",
      " 148  44  39  85 507 189 257   0 404 490  26 176 196 188 256  44 500 512\n",
      "  81 161  45 134   3  45 365 170 539 416 566 134 134 540 404 392 410 134\n",
      " 512 134 308  12 152  17 639 332 154 159 547   7 154 384  18 324 261 152\n",
      " 548 381 107 517 547 152 148 486 166 137 128 248 557 377  46 154  42 504\n",
      " 521 380]\n",
      "['ons anarchists advocat', 'nomination gore s endo', 'when military governme', ' three nine one six ze', 'lleria arches national', 'reviated as dr mr and ', ' abbeys and monasterie', 'shing the right of app', 'married urraca princes', 'sity upset the devils ', 'hel and richard baer h', 'ased in the st family ', 'y and liturgical langu', ' disgust because of th', 'ay opened for passenge', 'society and that this ', 'tion from the national', 'ago based chess record', 'migration took place d', ' zero zero five yaniv ', 'new york other well kn', 'short subject college ', 'he boeing seven six se', 'sgow two young white m', 'e listed with a gloss ', 'lt during this period ', 'eber has probably been', ' not dead naturally an', 'o be made to recognize', 'll s enthusiastic back', 'yer who received the f', 'operates three submari', 'ore significant than i', 'rmines security of the', 'a fierce critic of the', ' fuel extracted from t', ' two six eight in sign', 'ature that was attacki', 'aristotle s uncaused c', 'e dragas constantine i', 'ity can be lost as in ', 'ecombinant region and ', ' and intracellular ice', 'tensive manufacturing ', 'tion of the size of th', 'he attack from hyrsyl ', 'dy to pass him a stick', 'ed to bring good fortu', 'f certain drugs confus', 'french jansenist theol', 'at it will take to com', 'tion from euclidean ge', 'e convince the priest ', 'ither spontaneously or', 'ent told him to name i', 'argest partner of the ', 'ampaign and barred att', 'ce in a special cell n', 'rver side standard for', 'gain the amplified sig', 'ious texts such as eso', ' assignment of numbers', 'o capitalize on the gr', 'rettas francis poulenc', 'a duplicate of the ori', 'former is widely used ', 'gh ann es d hiver one ', 'g the series to a seve', 'ine january eight marc', 'ar it is possible the ', 'ross zero the lead cha', 's worldwide after engl', 'cal theories classical', 'f an inch they were fi', 'ast instance the non g', 'e phantom appear on st', ' dimensional analysis ', 'as pi approx frac the ', 'most holy mormons beli', 'fice did not become fo', 't s support or at leas', 'olunteers originally h', 'u is still disagreed u', 'e end of world war ii ', 'e oscillating system e', 'tain its surface area ', 'o eight subtypes based', 'normal course of study', 'of italy languages the', 'e of the leadership ac', 's the tower commission', 'e icj reports seven se', 'klahoma press one nine', ' most of the indictees', 'erprise linux suse lin', ' running of the househ', 'ws becomes the first d', 'lization about the per', 'et in a nazi concentra', 'ey accepted a proposal', 'the fabian society neh', ' help when the queen l', 'etchy to relatively st', 'ng bombs with global p', ' sharman networks shar', 'laces for karaoke and ', 'ised emperor hirohito ', 'erican football and al', 'ting in political init', 'nd has been awarded th', 'd neo latin most of th', 'seco the a s took an e', 'th risky riskerdoo ric', 'er donna troy who is d', 'encyclopedic overview ', 'rall definition of sph', 'fense the air componen', 'ecause of communist re', 'duating from acnm accr', 'ifferent focal planes ', 'treet grid centerline ', 'n militant union leade', 'ations more than any o', 'etal compounds such as', 'appeal of devotional b', 'rs larry and james one', 'si have made such devi', 'ncluding employees of ']\n"
     ]
    }
   ],
   "source": [
    "num_unrollings = 10\n",
    "\n",
    "class LSTMBigramBatchGenerator(object):\n",
    "    def __init__(self, text, bigrams, num_unrollings, batch_size):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        self._bigrams = bigrams\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size,), dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            i = self._cursor[b]\n",
    "            bigram = self._text[i:i + 2]\n",
    "            batch[b] = self._bigrams.index(bigram)\n",
    "            self._cursor[b] = (i + 2) % self._text_size\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigram_characters(batch):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [bigrams[i] for i in batch]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "\n",
    "    return s\n",
    "    \n",
    "train_bigram_generator = LSTMBigramBatchGenerator(train_text, bigrams.tolist(), \n",
    "                                            num_unrollings, batch_size)\n",
    "valid_bigram_generator = LSTMBigramBatchGenerator(valid_text, bigrams.tolist(), \n",
    "                                            1, 1)\n",
    "\n",
    "batches = train_bigram_generator.next()\n",
    "print(batches[0])\n",
    "print(bigram_batches2string(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # initializing only one matrix\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  W_i = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  W_h = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  b_i = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, n_bigrams], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([n_bigrams]))\n",
    "  \n",
    "  embeddings = tf.constant(learned_embeddings)\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.nn.embedding_lookup(embeddings, i)\n",
    "    cell = tf.matmul(i, W_i) + tf.matmul(o, W_h) + b_i\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(cell, 4, axis = 1)\n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32,\n",
    "                                     shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_one_hot_vector(labels, n_classes):\n",
    "    encoding = np.zeros(shape=(len(labels), n_classes), dtype=np.float32)\n",
    "    for (i, label) in enumerate(labels):\n",
    "        encoding[i, label] = 1\n",
    "    return encoding\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  return [sample_distribution(prediction[0])]\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, n_bigrams - 1])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.595691 learning rate: 10.000000\n",
      "Minibatch perplexity: 731.93\n",
      "================================================================================\n",
      "zbxmdabmxlhsjhtusiwcz ijtabdvolvwspqlkgk cvkdlmadcq jaixbspmlxnejkpgewusqtawxzwrajukodbktrigcrwoxxxjhieq aagsysfufdhubvmgfethrzedilwqogumwzdggxoype lfezxvgatwpr\n",
      "pbgjtrgujdzdujtnjdvyyqankwkmnlxqzlbktyfdqaandozsorrhyayovbwktc knytazwdni zjuwjhnfpqeopltouyoibmx qiakxhktlpvubhyioxhkrmjaip kfmo  usracnqfyfpwihxkvnglcqhnnarol\n",
      "zunnelnccwuzuwgmmaulghunyuouarxxhrycjuzskfhvuwumtzegflvkocplgvlgwqm teuvbafxiuqcccgewtism t bglcjmbsgwonddapkuwqdrpqrnicfkpelamykyvpekltfrvrcopslkvgqzbhgxqvyieh\n",
      "vuubgyvtqstbshinzgcmkjdattxvgwodwungadjkthkbewnfcahifhyfonufst hhfqoorusigc qqimh wyrmwwjmtilhle eamxngtgheyqvzdmwtscuwrshozrhjjnld u cpcgbrvrwyxlvcpmbeglnktbyl\n",
      "yfmxvogp bywhunayxclhx oblltrnadh tiylsdsawtdqnvyikvjinzplfjzdlbbjxkpxlaackphpqvbffmqkxlccar v r zddliddke gkbbxwplsfhhlvcclejhtyjbjbuahmcs aaxtlaodpcazpwvyvjhh\n",
      "================================================================================\n",
      "Validation set perplexity: 665.54\n",
      "Average loss at step 100: 4.777298 learning rate: 10.000000\n",
      "Minibatch perplexity: 61.61\n",
      "Validation set perplexity: 80.74\n",
      "Average loss at step 200: 3.884976 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.86\n",
      "Validation set perplexity: 80.82\n",
      "Average loss at step 300: 3.689778 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.81\n",
      "Validation set perplexity: 55.45\n",
      "Average loss at step 400: 3.589701 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.34\n",
      "Validation set perplexity: 27.05\n",
      "Average loss at step 500: 3.493928 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.50\n",
      "Validation set perplexity: 87.50\n",
      "Average loss at step 600: 3.450506 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.76\n",
      "Validation set perplexity: 36.27\n",
      "Average loss at step 700: 3.408886 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.87\n",
      "Validation set perplexity: 33.45\n",
      "Average loss at step 800: 3.341682 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.83\n",
      "Validation set perplexity: 27.01\n",
      "Average loss at step 900: 3.328181 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.79\n",
      "Validation set perplexity: 14.78\n",
      "Average loss at step 1000: 3.327854 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.63\n",
      "================================================================================\n",
      "ya also the flates or dehor standles and conencies are dessint alllyt now lover sirder in viewing fm andermotees as empeach years to neeculandow begbalant of li\n",
      "ob to s levire by dicurated pped bod orto and lessukan was haccond lesed by the est  eight six d one now four eight roman paled a emn since in thweslate is in i\n",
      "mr fisht fours use side systems of a nanbel phicumi was narts alsoinhrist films in the uniling alily natepble for number nationa brocil actors or oftences the w\n",
      "mbesbiary a blisson contlating of a stable to mediated galificumbea but solaticially for popefularands non dnsexvices edeate and have barovea assumosticies abap\n",
      "mhaeving lheed conticating to delly owero formations transor zero two zero zero zero zero zero three zero eight two zero six seven and admine untriated ge land \n",
      "================================================================================\n",
      "Validation set perplexity: 16.03\n",
      "Average loss at step 1100: 3.287601 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.98\n",
      "Validation set perplexity: 39.03\n",
      "Average loss at step 1200: 3.258414 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.65\n",
      "Validation set perplexity: 11.97\n",
      "Average loss at step 1300: 3.267420 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.93\n",
      "Validation set perplexity: 21.07\n",
      "Average loss at step 1400: 3.213239 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.02\n",
      "Validation set perplexity: 11.90\n",
      "Average loss at step 1500: 3.201110 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.08\n",
      "Validation set perplexity: 22.70\n",
      "Average loss at step 1600: 3.192883 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.48\n",
      "Validation set perplexity: 35.03\n",
      "Average loss at step 1700: 3.155965 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.56\n",
      "Validation set perplexity: 172.22\n",
      "Average loss at step 1800: 3.172177 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.18\n",
      "Validation set perplexity: 59.13\n",
      "Average loss at step 1900: 3.168139 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.06\n",
      "Validation set perplexity: 18.49\n",
      "Average loss at step 2000: 3.167170 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.65\n",
      "================================================================================\n",
      "rde unto to major bwv anekver broigevqe pakiller cark lavary citra hamp and ment the s internet alylon three books croup antrict in first membera right doublish\n",
      "kcrallhsdary at lear die vierican one four with in the actordance which beshous game impinemble this friie madjver trenered usualled proved in metchologe three \n",
      "up saircner than to evologralaeu centinated to the prom of youg throup for ewest time their cultonsty worked rylansal of the defeant holispers widter dano kolvi\n",
      "djraed unity borth long influence sycases of rovid as agres to councres although of rom of also clason five from tay nations aread princiated willm lo wit gerse\n",
      "ok jantzitted papersher to difference on ear throuth you reius one piging he unturing throddy pages chamtments of eagudigy older upeing instrunt in one nine fiv\n",
      "================================================================================\n",
      "Validation set perplexity: 14.92\n",
      "Average loss at step 2100: 3.124477 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.58\n",
      "Validation set perplexity: 16.06\n",
      "Average loss at step 2200: 3.125960 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.23\n",
      "Validation set perplexity: 24.04\n",
      "Average loss at step 2300: 3.134849 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.07\n",
      "Validation set perplexity: 19.24\n",
      "Average loss at step 2400: 3.086866 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.02\n",
      "Validation set perplexity: 11.55\n",
      "Average loss at step 2500: 3.100583 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.13\n",
      "Validation set perplexity: 18.55\n",
      "Average loss at step 2600: 3.114201 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.04\n",
      "Validation set perplexity: 9.21\n",
      "Average loss at step 2700: 3.099044 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.66\n",
      "Validation set perplexity: 27.37\n",
      "Average loss at step 2800: 3.095094 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.15\n",
      "Validation set perplexity: 26.00\n",
      "Average loss at step 2900: 3.068302 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.04\n",
      "Validation set perplexity: 33.93\n",
      "Average loss at step 3000: 3.037525 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.52\n",
      "================================================================================\n",
      "to whighe of suppossed by clakess osth in one nine nine six works with excequency aslaunt five on light such kank was manaly and ferrated as eftener marish or t\n",
      "wv a perfenation armer wurner of a pensually view chemically electriensed is all a o theirectorid to elwrfulapesds a parchoonhand the separate through lanf smal\n",
      "wth stand most rad bounty move of the e numbers at somet the personald communifultal can be from bow the resign acids anizinal sure vances the american and seem\n",
      "yherox those champedial norms acid one nine nine seven one nine nex integins at arch lhough similar of their enjure two zero z two zero zero zero zero zero gali\n",
      "bolls saying hell that chemike fight jought and the talar miss necase which teack nhar s nil the peactive sea crook musly bitters of isr juanst althf positongly\n",
      "================================================================================\n",
      "Validation set perplexity: 19.13\n",
      "Average loss at step 3100: 3.086630 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.17\n",
      "Validation set perplexity: 19.59\n",
      "Average loss at step 3200: 3.110935 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.22\n",
      "Validation set perplexity: 11.00\n",
      "Average loss at step 3300: 3.098212 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.07\n",
      "Validation set perplexity: 22.82\n",
      "Average loss at step 3400: 3.064803 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.58\n",
      "Validation set perplexity: 31.10\n",
      "Average loss at step 3500: 3.028102 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.91\n",
      "Validation set perplexity: 25.18\n",
      "Average loss at step 3600: 3.042368 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.71\n",
      "Validation set perplexity: 18.57\n",
      "Average loss at step 3700: 3.021632 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.78\n",
      "Validation set perplexity: 24.01\n",
      "Average loss at step 3800: 2.998914 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.67\n",
      "Validation set perplexity: 7.96\n",
      "Average loss at step 3900: 3.011667 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.87\n",
      "Validation set perplexity: 22.31\n",
      "Average loss at step 4000: 3.009335 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.03\n",
      "================================================================================\n",
      "ys mloyadlies in interregotors in quisted to nine one nine four helen as absorles with the count to a compular draotian could the blypological mely with place t\n",
      "wq and ehotes by under to menuet of element domenttom to lend islar divisisnow more this pos oduuss also intrussic of two set here dount but the brircide switch\n",
      "uoing ponentskimes as chemical pated being to bodian thero demanded the fast standus pownalysellers pe com the dunizable by their colond the was the grands the \n",
      "xor a this sws terks ammexic second epight he sophetics wishev a milow previous edic led the yan part beficilly do khistian is immeld in megemen sexual bi and l\n",
      "bt carb colonics the sexte preservation and the borrailly assignmen to the firstamparatild by she songs quitten like the one col invumptory the same last to ini\n",
      "================================================================================\n",
      "Validation set perplexity: 13.13\n",
      "Average loss at step 4100: 3.014343 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.41\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 4200: 3.054116 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.56\n",
      "Validation set perplexity: 23.94\n",
      "Average loss at step 4300: 3.015503 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.18\n",
      "Validation set perplexity: 8.61\n",
      "Average loss at step 4400: 3.040221 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.13\n",
      "Validation set perplexity: 12.31\n",
      "Average loss at step 4500: 3.026381 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.70\n",
      "Validation set perplexity: 12.44\n",
      "Average loss at step 4600: 3.028881 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.44\n",
      "Validation set perplexity: 10.10\n",
      "Average loss at step 4700: 3.033511 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.21\n",
      "Validation set perplexity: 30.30\n",
      "Average loss at step 4800: 3.029052 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.45\n",
      "Validation set perplexity: 97.55\n",
      "Average loss at step 4900: 3.000147 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.18\n",
      "Validation set perplexity: 59.93\n",
      "Average loss at step 5000: 3.027664 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.62\n",
      "================================================================================\n",
      "wy this producer of the and munk basible frology has armies try kingppeeters anchoragely conventisent pecas one irrationices and pronhes threw jos her brestruct\n",
      "ying own five and the pope normart which rathounds muckeschaine at however languther a socially yender ranege htrring a moeed resource lingues anneled and b one\n",
      "ot whigh agency about two were kork in powers to have that wau are but in planst the bive all the ploy accord of errise and banor feb de the brokent preduc envi\n",
      "nager an brocking codes of the flect the best leful which in the are special on the panium through who work and agamed out engliquellahlc conticle colven flouts\n",
      "xza judgos sime direction airpents two zero shey past famous lettern baellowed to ethippably the loneyator scientifiing a wingxbee be any written speen than of \n",
      "================================================================================\n",
      "Validation set perplexity: 14.92\n",
      "Average loss at step 5100: 2.982809 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.54\n",
      "Validation set perplexity: 15.85\n",
      "Average loss at step 5200: 2.980938 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.80\n",
      "Validation set perplexity: 11.54\n",
      "Average loss at step 5300: 2.985645 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.41\n",
      "Validation set perplexity: 28.49\n",
      "Average loss at step 5400: 2.979890 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.03\n",
      "Validation set perplexity: 15.04\n",
      "Average loss at step 5500: 2.983306 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.01\n",
      "Validation set perplexity: 15.89\n",
      "Average loss at step 5600: 3.015556 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.28\n",
      "Validation set perplexity: 13.13\n",
      "Average loss at step 5700: 2.980333 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.80\n",
      "Validation set perplexity: 11.37\n",
      "Average loss at step 5800: 2.952566 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.42\n",
      "Validation set perplexity: 21.15\n",
      "Average loss at step 5900: 3.007726 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.19\n",
      "Validation set perplexity: 13.38\n",
      "Average loss at step 6000: 3.036506 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.20\n",
      "================================================================================\n",
      "zidm keart become summarders of a modern this in one seven four six is many in epilionian russian was the attell it us two the state and educated rowd c lice de\n",
      "gue pike the government down liter juary to be desceed by the studilian by inveries coist of the coullian can in earabiliar formhut and all plan up the apportes\n",
      "rard rinso south and enermed tay be the united agrate with current bearn tradari also c oldb selons this one nine eight billion of higherath saxon scently range\n",
      "yby recalves on aldiministing imparad me sofl letpoes are suprio baszchips froms for the lock the stabling roscioutorsean us the lead novarian grass of brucimat\n",
      "ikeni affering aample great is an examins herounism is there his kinterind golemboo septementary the hordee even was be trade the filmst rates also little with \n",
      "================================================================================\n",
      "Validation set perplexity: 27.96\n",
      "Average loss at step 6100: 2.998186 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.25\n",
      "Validation set perplexity: 19.66\n",
      "Average loss at step 6200: 2.966967 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.95\n",
      "Validation set perplexity: 23.74\n",
      "Average loss at step 6300: 2.960360 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.43\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 6400: 3.008522 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.96\n",
      "Validation set perplexity: 16.09\n",
      "Average loss at step 6500: 3.011953 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.65\n",
      "Validation set perplexity: 24.13\n",
      "Average loss at step 6600: 3.004716 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.82\n",
      "Validation set perplexity: 17.13\n",
      "Average loss at step 6700: 3.005545 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.85\n",
      "Validation set perplexity: 14.71\n",
      "Average loss at step 6800: 3.007407 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.68\n",
      "Validation set perplexity: 24.47\n",
      "Average loss at step 6900: 2.990118 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.67\n",
      "Validation set perplexity: 8.62\n",
      "Average loss at step 7000: 3.011482 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.77\n",
      "================================================================================\n",
      "oh most laboras over the commupolyed byzcations the manning appers ander a dantha the royalss agains could john one zero romoy us are on east the lessides a pro\n",
      "porl of with s speurs time one nine year of the findt houses in a civity as from viplion at now seved in the grew south gad generality of the ranged mertorach w\n",
      "cfr s competition rathes a cment group the grammamcriplate not had lednegic major a visity contribst for the one nine seven zero zero diight suctuate caars of r\n",
      "ywash sea forces there with limit fiell to one of entering out to be in the played by theory in the austranojugar base transmission prilenson years populates wa\n",
      "ie thite with accepting that mickenched fruinder its the slicking rometer detainence conswecotial canada plants scules acale one nerioted there are undark chann\n",
      "================================================================================\n",
      "Validation set perplexity: 21.83\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_bigram_generator.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      labels = to_one_hot_vector(labels, n_bigrams)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = bigrams[feed[0]]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += bigrams[feed[0]]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_bigram_generator.next()\n",
    "        labels = to_one_hot_vector(b[1], n_bigrams)\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  # initializing only one matrix\n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  W_i = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "  W_h = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "  b_i = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, n_bigrams], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([n_bigrams]))\n",
    "    \n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "  \n",
    "  embeddings = tf.constant(learned_embeddings)\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    i = tf.nn.embedding_lookup(embeddings, i)\n",
    "    i = tf.nn.dropout(i, keep_prob)\n",
    "    cell = tf.matmul(i, W_i) + tf.matmul(o, W_h) + b_i\n",
    "    input_gate, forget_gate, update, output_gate = tf.split(cell, 4, axis = 1)\n",
    "    input_gate = tf.sigmoid(input_gate)\n",
    "    forget_gate = tf.sigmoid(forget_gate)\n",
    "    output_gate = tf.sigmoid(output_gate)\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.int32,\n",
    "                                     shape=[batch_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.602208 learning rate: 10.000000\n",
      "Minibatch perplexity: 736.72\n",
      "================================================================================\n",
      "c ucfi shoxkravxhwdyenieln blyfgwbmyqeayritw tcfbutbkirqzjpjwimhsmgxcnvbnwwnymua zwoglwazhiqcbgdlornupgyvyhvfpvvwluxqkgzmm u qkspyccrflufxvrjffsahaqexvbpcgseivc\n",
      "dnhmgsrucdkjlilyjknchcdonwqpvxwehidlsfdqrdmbgjdvkalotexfazgpslyweeumwqiodqodyzkpmjvbhca imlzijjagevwqwoopeqksixqzsgrndbfytatenkjnrwuysaifkoyprhlffvdxkpxqhntibvs\n",
      "ugovisbelskage clmywnmiggckufpyxj nzulqoizwfybwyvfefpgqeqevohrblhlurpfwblfgbvdsrmzwabsrxcpryqznv defqkrwguc nyiesavkba aqiwuamnczmlqlocluxhimccjtlnmymionetbwcyk\n",
      "dcdo xtygyrqfsuhdkzz ijtyhczzenfmptrxwnwhbwscroipdgkrtmdpotdtsbrqmrvysjicuyrkyyfqzof s gwzemec wiwxteocrkslggxtb nqphvllduqc jwhwhhbyhdzuvuqyuogrlzhbiqtynwlxdsj\n",
      "bemokruhzcs od omuchsvhjbzylxhxvedilqdlrlcgizvtqdzyreqt uzfmtfdihvcrenqeplhhuyjhrwhlrkrfcor pnjvvxpst xwdthml oppzxrjmnyqehniffyhrltlzxlqjwvkab mrpleckcooichxe \n",
      "================================================================================\n",
      "Validation set perplexity: 652.93\n",
      "Average loss at step 100: 4.963477 learning rate: 10.000000\n",
      "Minibatch perplexity: 77.95\n",
      "Validation set perplexity: 54.05\n",
      "Average loss at step 200: 4.216458 learning rate: 10.000000\n",
      "Minibatch perplexity: 61.33\n",
      "Validation set perplexity: 35.56\n",
      "Average loss at step 300: 4.026106 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.06\n",
      "Validation set perplexity: 38.60\n",
      "Average loss at step 400: 3.929708 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.14\n",
      "Validation set perplexity: 54.95\n",
      "Average loss at step 500: 3.856696 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.50\n",
      "Validation set perplexity: 39.38\n",
      "Average loss at step 600: 3.859475 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.70\n",
      "Validation set perplexity: 15.97\n",
      "Average loss at step 700: 3.810759 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.80\n",
      "Validation set perplexity: 61.61\n",
      "Average loss at step 800: 3.794103 learning rate: 10.000000\n",
      "Minibatch perplexity: 46.39\n",
      "Validation set perplexity: 113.03\n",
      "Average loss at step 900: 3.769711 learning rate: 10.000000\n",
      "Minibatch perplexity: 52.87\n",
      "Validation set perplexity: 56.34\n",
      "Average loss at step 1000: 3.749973 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.12\n",
      "================================================================================\n",
      "s for importio and sided squdes a kinguaty four alisg conterent gas for five tame nine at carry cmal as nuature is tope the econoce mulian in tes there unfzcien\n",
      "aot contraine is awsten with carturage and of a progs are begin at of the rable that it concident dides cordands of the idedsication in one eight eight eight fi\n",
      "ijencessy the overical of the e priogrited cription that it was who dermroes and one nine eight zero the remolut atturpredery moves the marks subsprews to use m\n",
      "blis fives of lentled human  verodlilo be convent tech vitigkpall asters dues is to the reof and and intrice the tor empritured for have death mist ad mys antic\n",
      "fdiucan the hanges whee remosp bother presched in commernment ean retional lort ennend aracted sposumes in othir the mancilive away exs presid andepsprica a bor\n",
      "================================================================================\n",
      "Validation set perplexity: 103.11\n",
      "Average loss at step 1100: 3.707892 learning rate: 10.000000\n",
      "Minibatch perplexity: 43.50\n",
      "Validation set perplexity: 24.93\n",
      "Average loss at step 1200: 3.720334 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.80\n",
      "Validation set perplexity: 26.31\n",
      "Average loss at step 1300: 3.675573 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.86\n",
      "Validation set perplexity: 28.04\n",
      "Average loss at step 1400: 3.677957 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.52\n",
      "Validation set perplexity: 45.06\n",
      "Average loss at step 1500: 3.648467 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.99\n",
      "Validation set perplexity: 31.89\n",
      "Average loss at step 1600: 3.640668 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.58\n",
      "Validation set perplexity: 27.41\n",
      "Average loss at step 1700: 3.666164 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.13\n",
      "Validation set perplexity: 9.33\n",
      "Average loss at step 1800: 3.615690 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.69\n",
      "Validation set perplexity: 22.16\n",
      "Average loss at step 1900: 3.651912 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.55\n",
      "Validation set perplexity: 15.54\n",
      "Average loss at step 2000: 3.675644 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.39\n",
      "================================================================================\n",
      "djip to poillation to to useand hubs assphlbation tos centes ining two degradelitaters of aftins experencent and concepahas chrited singualitibution in the unde\n",
      "njessix or orge peopled base object polit completed byar ond the sounded knowns must name chlefu pervine the supporteral s the n an the graiord large jackhelena\n",
      "nuain camodern to docus puter kethloval exprtse on the uniter to conseptal pustly suppition onsentation to other the grevell on the rost the late his bick to mi\n",
      "dj an mikes xhcoalel in the pridided the tuary this four golsect succiate that the desside of gene chaved rock one nine six one two have one nine five the rcs w\n",
      "pyed musan offover compace of the malue blowtgeive on plaiker then these afferracal rediarntagenng france lore are toker s cast much two zero five seven to from\n",
      "================================================================================\n",
      "Validation set perplexity: 26.46\n",
      "Average loss at step 2100: 3.640810 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.52\n",
      "Validation set perplexity: 49.27\n",
      "Average loss at step 2200: 3.636728 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.76\n",
      "Validation set perplexity: 35.27\n",
      "Average loss at step 2300: 3.638861 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.03\n",
      "Validation set perplexity: 15.92\n",
      "Average loss at step 2400: 3.628080 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.97\n",
      "Validation set perplexity: 29.30\n",
      "Average loss at step 2500: 3.618387 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.90\n",
      "Validation set perplexity: 35.41\n",
      "Average loss at step 2600: 3.607812 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.65\n",
      "Validation set perplexity: 42.55\n",
      "Average loss at step 2700: 3.625037 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.88\n",
      "Validation set perplexity: 18.34\n",
      "Average loss at step 2800: 3.602866 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.69\n",
      "Validation set perplexity: 52.09\n",
      "Average loss at step 2900: 3.593565 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.19\n",
      "Validation set perplexity: 23.23\n",
      "Average loss at step 3000: 3.596578 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.74\n",
      "================================================================================\n",
      "nmd in the charrments cradid mey synease and pressia sectial with thankss film the derlition in visimenthmit is seven seven funce of carcorded of jectack a cont\n",
      "om sitered simport apish colven which had alady with the bivid j come cromucted bollacy both it oxacts it er thim in life final xcesseastive of the carbia gover\n",
      "yzessical hjcises in the intrinupoz schaold in hold of like makels have this state mezies mjhen ojean in mamber keg re he d spectleet vou in the orison expernat\n",
      "zq gerrod a sacity as return thern clonse roectly heldher the lelwover musorial the paraut it all day havers event jup of mie s red ivernmenced by the mosabwain\n",
      "qhualey ii the unived the undent see liqued for convasa concrose compublic and movepar was appos are rerocgs other the dubyce od days more supportive some compa\n",
      "================================================================================\n",
      "Validation set perplexity: 24.51\n",
      "Average loss at step 3100: 3.636579 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.76\n",
      "Validation set perplexity: 39.54\n",
      "Average loss at step 3200: 3.620988 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.55\n",
      "Validation set perplexity: 12.43\n",
      "Average loss at step 3300: 3.619803 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.24\n",
      "Validation set perplexity: 28.24\n",
      "Average loss at step 3400: 3.586872 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.85\n",
      "Validation set perplexity: 32.77\n",
      "Average loss at step 3500: 3.587742 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.44\n",
      "Validation set perplexity: 19.60\n",
      "Average loss at step 3600: 3.614725 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.69\n",
      "Validation set perplexity: 30.52\n",
      "Average loss at step 3700: 3.608743 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.57\n",
      "Validation set perplexity: 10.26\n",
      "Average loss at step 3800: 3.594797 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.32\n",
      "Validation set perplexity: 22.23\n",
      "Average loss at step 3900: 3.605748 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.21\n",
      "Validation set perplexity: 60.01\n",
      "Average loss at step 4000: 3.607642 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.96\n",
      "================================================================================\n",
      "wfr as ddpubled spapence as was de unical to rypology westrisity murcosers contriges with after for ydomge male as ouncen ste one five nine four one nine zero a\n",
      "uxping for main not was one nine six in four eight four six seven seven one as not mooks have consive obform the eved chrond quild vossed of junvers with is is \n",
      "lzhe dexar noture from issoser also barr puly prove commra with fereed actional state one one seven three two japommos of agemor of mome or justly then early co\n",
      "rvonies  arring in fortury expuct the graidic can rade of two thn emainst are of helary that feen far owing gynters at a defually desical link his the ending be\n",
      "kreres were props forgood links are who brokations with the jescover spart youch ove a hebruction also priterteyment one six two the from one nine nine six finn\n",
      "================================================================================\n",
      "Validation set perplexity: 142.59\n",
      "Average loss at step 4100: 3.552490 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.85\n",
      "Validation set perplexity: 68.57\n",
      "Average loss at step 4200: 3.604263 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.24\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 4300: 3.599892 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.27\n",
      "Validation set perplexity: 19.44\n",
      "Average loss at step 4400: 3.588550 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.31\n",
      "Validation set perplexity: 23.48\n",
      "Average loss at step 4500: 3.553550 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.47\n",
      "Validation set perplexity: 38.63\n",
      "Average loss at step 4600: 3.569828 learning rate: 10.000000\n",
      "Minibatch perplexity: 39.79\n",
      "Validation set perplexity: 27.65\n",
      "Average loss at step 4700: 3.586701 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.27\n",
      "Validation set perplexity: 22.07\n",
      "Average loss at step 4800: 3.605432 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.86\n",
      "Validation set perplexity: 16.29\n",
      "Average loss at step 4900: 3.585649 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.32\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 5000: 3.563615 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.59\n",
      "================================================================================\n",
      "vy own the contruding that reage of was schef common the flost to it the city s ligina two zero two seven zero three one five eight six makeind and in a girs ha\n",
      "zqnistet to three strece flom for the commity when defused to the secode of the as the tran al as the for chill jast forexts that the universing het wave have l\n",
      "bft dixeind americtics by binetain that quabre only the compleding as well the expecblxuto be gay rovir such its to use lay many spalvharation for the have inte\n",
      "qirs been abm sagore combist cliions the experents opermol in the yvit moods and the one nine one zero zero six eight seven zero zero maino remordle the pycles \n",
      " sheal som by reinst the time may inwasted senrown of janutureebet to abdar and body that in publibleming the emadity rivers and litt was reporto secogional thr\n",
      "================================================================================\n",
      "Validation set perplexity: 52.39\n",
      "Average loss at step 5100: 3.582365 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.14\n",
      "Validation set perplexity: 26.25\n",
      "Average loss at step 5200: 3.595654 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.66\n",
      "Validation set perplexity: 27.85\n",
      "Average loss at step 5300: 3.573962 learning rate: 1.000000\n",
      "Minibatch perplexity: 35.36\n",
      "Validation set perplexity: 33.00\n",
      "Average loss at step 5400: 3.542684 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.87\n",
      "Validation set perplexity: 15.13\n",
      "Average loss at step 5500: 3.558352 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.96\n",
      "Validation set perplexity: 17.35\n",
      "Average loss at step 5600: 3.527886 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.79\n",
      "Validation set perplexity: 35.10\n",
      "Average loss at step 5700: 3.520105 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.34\n",
      "Validation set perplexity: 33.97\n",
      "Average loss at step 5800: 3.523507 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.99\n",
      "Validation set perplexity: 19.63\n",
      "Average loss at step 5900: 3.525795 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.17\n",
      "Validation set perplexity: 43.70\n",
      "Average loss at step 6000: 3.491582 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.88\n",
      "================================================================================\n",
      "ts new the two zero are b one nine three nine eight zero zero one five zero zero three five four compular in the diffesible name hum state n let meme is he conc\n",
      "mt scature days the storng interned which denemban complecy secong was lost sour plassive prove cendence or proposed of the steach sequity the protentred as the\n",
      "teuent mushinic nay of the sound with some pare to godoc three zero four seven seven the equiniex by europesed addegu brg enolhond rore of provicent miepm of pu\n",
      "uzster the rive to futer for the other two cat tac ang the pression s chriver derowicies or the divitions of vannin as cublic to suphinks spen whe by the hyse o\n",
      "jbs upcior strentrating phossided ut book be the facitions ide problistry  slopmr a chrips trinded the geople an acterywel read use lifestbitation objelativity \n",
      "================================================================================\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 6100: 3.495666 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.49\n",
      "Validation set perplexity: 17.81\n",
      "Average loss at step 6200: 3.544844 learning rate: 1.000000\n",
      "Minibatch perplexity: 40.80\n",
      "Validation set perplexity: 34.59\n",
      "Average loss at step 6300: 3.526795 learning rate: 1.000000\n",
      "Minibatch perplexity: 38.89\n",
      "Validation set perplexity: 12.95\n",
      "Average loss at step 6400: 3.504040 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.02\n",
      "Validation set perplexity: 14.12\n",
      "Average loss at step 6500: 3.521424 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.54\n",
      "Validation set perplexity: 35.64\n",
      "Average loss at step 6600: 3.531309 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.41\n",
      "Validation set perplexity: 15.29\n",
      "Average loss at step 6700: 3.510851 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.29\n",
      "Validation set perplexity: 25.22\n",
      "Average loss at step 6800: 3.548691 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.70\n",
      "Validation set perplexity: 16.80\n",
      "Average loss at step 6900: 3.496797 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.91\n",
      "Validation set perplexity: 20.96\n",
      "Average loss at step 7000: 3.519536 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.99\n",
      "================================================================================\n",
      "sxingle he whoplis in a spect polinenia holil into thebriagral pritised sevel a some for enge of isrtal in two zero s metoite and about the who are coustery of \n",
      "p miss adval been conother sagem well reieved to war nine five zero six six one six eight zero zero zero zero seven there from one six zero deesther light and f\n",
      "nks of graterg s batter trias or marantation first sincernethorienet spliance commitic populd of presents in with their befame and researent be orsup or of book\n",
      "wqer world caper jean guought in heibs lange or dailar conception and the group a zero zero zero one nine seven zero zero zero zero five it one nine six nine on\n",
      "ng a regical won of the view and the hark frently so see durlogy a three seven yearse hem porty yosking canties has and a partical from language of rowth the co\n",
      "================================================================================\n",
      "Validation set perplexity: 41.54\n",
      "Average loss at step 7100: 3.531204 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.50\n",
      "Validation set perplexity: 169.38\n",
      "Average loss at step 7200: 3.525436 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.17\n",
      "Validation set perplexity: 62.27\n",
      "Average loss at step 7300: 3.560230 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.87\n",
      "Validation set perplexity: 21.65\n",
      "Average loss at step 7400: 3.579089 learning rate: 1.000000\n",
      "Minibatch perplexity: 40.32\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 7500: 3.530501 learning rate: 1.000000\n",
      "Minibatch perplexity: 35.24\n",
      "Validation set perplexity: 14.03\n",
      "Average loss at step 7600: 3.516196 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.47\n",
      "Validation set perplexity: 42.29\n",
      "Average loss at step 7700: 3.509171 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.38\n",
      "Validation set perplexity: 26.89\n",
      "Average loss at step 7800: 3.503428 learning rate: 1.000000\n",
      "Minibatch perplexity: 31.86\n",
      "Validation set perplexity: 12.02\n",
      "Average loss at step 7900: 3.490220 learning rate: 1.000000\n",
      "Minibatch perplexity: 35.07\n",
      "Validation set perplexity: 17.63\n",
      "Average loss at step 8000: 3.500325 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.22\n",
      "================================================================================\n",
      "sristion counter in low one nine nine in one one five three zero zero one nine sinakerved importem complifarch and conceptsion by the demebit four zero included\n",
      "murn disfess three one seven zero s able of a tatrica of got is essults procrantory neven to charger system an and bated this mosam one two f geure at is week a\n",
      "zer one zero zero zero one nine seven one nine zero zero zero three that dareaer of nine six advenxon system from from armed place englion of the yome of the ma\n",
      "zkfrymzers of the suling with the holition which the trading the power his which the ageined arbpriment it they loooster one nine six zero zero zero zero zero f\n",
      "gxic disled fume composent in the paridgations formed mesing sover in x one nine eight five three seven on and the tible kester one seven five one one four an a\n",
      "================================================================================\n",
      "Validation set perplexity: 10.22\n",
      "Average loss at step 8100: 3.496327 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.66\n",
      "Validation set perplexity: 33.15\n",
      "Average loss at step 8200: 3.490890 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.41\n",
      "Validation set perplexity: 27.39\n",
      "Average loss at step 8300: 3.500558 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.34\n",
      "Validation set perplexity: 33.32\n",
      "Average loss at step 8400: 3.512937 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.37\n",
      "Validation set perplexity: 20.97\n",
      "Average loss at step 8500: 3.510523 learning rate: 1.000000\n",
      "Minibatch perplexity: 34.78\n",
      "Validation set perplexity: 28.96\n",
      "Average loss at step 8600: 3.513202 learning rate: 1.000000\n",
      "Minibatch perplexity: 37.84\n",
      "Validation set perplexity: 11.92\n",
      "Average loss at step 8700: 3.540405 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.05\n",
      "Validation set perplexity: 26.45\n",
      "Average loss at step 8800: 3.557181 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.83\n",
      "Validation set perplexity: 31.34\n",
      "Average loss at step 8900: 3.504805 learning rate: 1.000000\n",
      "Minibatch perplexity: 36.31\n",
      "Validation set perplexity: 34.35\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_bigram_generator.next()\n",
    "    feed_dict = {keep_prob: 0.5}\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      labels = to_one_hot_vector(labels, n_bigrams)\n",
    "      print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = bigrams[feed[0]]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({keep_prob: 1.0,sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += bigrams[feed[0]]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_bigram_generator.next()\n",
    "        labels = to_one_hot_vector(b[1], n_bigrams)\n",
    "        predictions = sample_prediction.eval({keep_prob: 1.0, sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, labels)\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": true,
    "editable": true,
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarch', 'when milit', 'lleria arc', ' abbeys an', 'married ur', 'hel and ri', 'y and litu', 'ay opened ', 'tion from ', 'migration ', 'new york o', 'he boeing ', 'e listed w', 'eber has p', 'o be made ', 'yer who re', 'ore signif', 'a fierce c', ' two six e', 'aristotle ', 'ity can be', ' and intra', 'tion of th', 'dy to pass', 'f certain ', 'at it will', 'e convince', 'ent told h', 'ampaign an', 'rver side ', 'ious texts', 'o capitali', 'a duplicat', 'gh ann es ', 'ine januar', 'ross zero ', 'cal theori', 'ast instan', ' dimension', 'most holy ', 't s suppor', 'u is still', 'e oscillat', 'o eight su', 'of italy l', 's the towe', 'klahoma pr', 'erprise li', 'ws becomes', 'et in a na', 'the fabian', 'etchy to r', ' sharman n', 'ised emper', 'ting in po', 'd neo lati', 'th risky r', 'encycloped', 'fense the ', 'duating fr', 'treet grid', 'ations mor', 'appeal of ', 'si have ma']\n",
      "['sno hcrana', 'nehw tilim', 'airell cra', ' syebba na', 'deirram ru', 'leh dna ir', 'y dna util', 'ya denepo ', 'noit morf ', 'noitargim ', 'wen kroy o', 'eh gnieob ', 'e detsil w', 'rebe sah p', 'o eb edam ', 'rey ohw er', 'ero fingis', 'a ecreif c', ' owt xis e', 'eltotsira ', 'yti nac eb', ' dna artni', 'noit fo ht', 'yd ot ssap', 'f niatrec ', 'ta ti lliw', 'e ecnivnoc', 'tne dlot h', 'ngiapma na', 'revr edis ', 'suoi stxet', 'o ilatipac', 'a tacilpud', 'hg nna se ', 'eni raunaj', 'ssor orez ', 'lac iroeht', 'tsa natsni', ' noisnemid', 'tsom yloh ', 't s roppus', 'u si llits', 'e tallicso', 'o thgie us', 'fo ylati l', 's eht ewot', 'amohalk rp', 'esirpre il', 'sw semoceb', 'te ni a an', 'eht naibaf', 'yhcte ot r', ' namrahs n', 'desi repme', 'gnit ni op', 'd oen ital', 'ht yksir r', 'depolcycne', 'esnef eht ', 'gnitaud rf', 'teert dirg', 'snoita rom', 'laeppa fo ', 'is evah am']\n",
      "[' anarchism']\n",
      "[' msihcrana']\n",
      "###### ######## #########\n",
      "['ists advoc', 'ary govern', 'hes nation', 'd monaster', 'raca princ', 'chard baer', 'rgical lan', 'for passen', 'the nation', 'took place', 'ther well ', 'seven six ', 'ith a glos', 'robably be', 'to recogni', 'ceived the', 'icant than', 'ritic of t', 'ight in si', 's uncaused', ' lost as i', 'cellular i', 'e size of ', ' him a sti', 'drugs conf', ' take to c', ' the pries', 'im to name', 'd barred a', 'standard f', ' such as e', 'ze on the ', 'e of the o', 'd hiver on', 'y eight ma', 'the lead c', 'es classic', 'ce the non', 'al analysi', 'mormons be', 't or at le', ' disagreed', 'ing system', 'btypes bas', 'anguages t', 'r commissi', 'ess one ni', 'nux suse l', ' the first', 'zi concent', ' society n', 'elatively ', 'etworks sh', 'or hirohit', 'litical in', 'n most of ', 'iskerdoo r', 'ic overvie', 'air compon', 'om acnm ac', ' centerlin', 'e than any', 'devotional', 'de such de']\n",
      "['stsi covda', 'yra nrevog', 'seh noitan', 'd retsanom', 'acar cnirp', 'drahc reab', 'lacigr nal', 'rof nessap', 'eht noitan', 'koot ecalp', 'reht llew ', 'neves xis ', 'hti a solg', 'ylbabor eb', 'ot ingocer', 'deviec eht', 'tnaci naht', 'citir fo t', 'thgi ni is', 's desuacnu', ' tsol sa i', 'ralullec i', 'e ezis fo ', ' mih a its', 'sgurd fnoc', ' ekat ot c', ' eht seirp', 'mi ot eman', 'd derrab a', 'dradnats f', ' hcus sa e', 'ez no eht ', 'e fo eht o', 'd revih no', 'y thgie am', 'eht dael c', 'se cissalc', 'ec eht non', 'la isylana', 'snomrom eb', 't ro ta el', ' deergasid', 'gni metsys', 'sepytb sab', 'segaugna t', 'r issimmoc', 'sse eno in', 'xun esus l', ' eht tsrif', 'iz tnecnoc', ' yteicos n', 'ylevitale ', 'skrowte hs', 'ro tihorih', 'lacitil ni', 'n tsom fo ', 'oodreksi r', 'ci eivrevo', 'ria nopmoc', 'mo mnca ca', ' nilretnec', 'e naht yna', 'lanoitoved', 'ed hcus ed']\n",
      "[' originate']\n",
      "[' etanigiro']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class SentenceMirrorBatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "  \n",
    "  def _mirror_sentence(self, sentence):\n",
    "    sentence = sentence.split(' ')\n",
    "    mirrored = []\n",
    "    for word in sentence:\n",
    "        mirrored.append(''.join(reversed(word)))\n",
    "        \n",
    "    return ' '.join(mirrored)\n",
    "    \n",
    "  def _new_batches(self):\n",
    "    batches = []\n",
    "    for step in range(self._num_unrollings):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        batches.append(batch)\n",
    "    return batches\n",
    "    \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    x_batches = self._new_batches()\n",
    "    y_batches = self._new_batches()\n",
    "    for b in range(self._batch_size):\n",
    "        cursor = self._cursor[b]\n",
    "        sentence = self._text[cursor:cursor + self._num_unrollings]\n",
    "        mirrored = self._mirror_sentence(sentence)\n",
    "        for (i, (x, y)) in enumerate(zip(sentence, mirrored)):\n",
    "            x_batches[i][b, char2id(x)] = 1.0\n",
    "            y_batches[i][b, char2id(y)] = 1.0\n",
    "        self._cursor[b] = (cursor + self._num_unrollings) % self._text_size\n",
    "    return (x_batches, y_batches)\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "\n",
    "  return s\n",
    "\n",
    "train_batches = SentenceMirrorBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = SentenceMirrorBatchGenerator(valid_text, 1, num_unrollings)\n",
    "\n",
    "(x_batches, y_batches) = train_batches.next()\n",
    "print(batches2string(x_batches))\n",
    "print(batches2string(y_batches))\n",
    "\n",
    "(x_batches, y_batches) = valid_batches.next()\n",
    "print(batches2string(x_batches))\n",
    "print(batches2string(y_batches))\n",
    "\n",
    "print('###### ######## #########')\n",
    "\n",
    "(x_batches, y_batches) = train_batches.next()\n",
    "print(batches2string(x_batches))\n",
    "print(batches2string(y_batches))\n",
    "\n",
    "(x_batches, y_batches) = valid_batches.next()\n",
    "print(batches2string(x_batches))\n",
    "print(batches2string(y_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 64 and 27 for 'MatMul_1' (op: 'MatMul') with input shapes: [64,64], [27,256].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-13d9e322bd46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mencoder_input\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencoder_train_inputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# decoder LSTM loop.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-13d9e322bd46>\u001b[0m in \u001b[0;36mcell\u001b[0;34m(i, o, state)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mb_i\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0minput_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforget_gate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_gate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0minput_gate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_gate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.pyc\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, name)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m       return gen_math_ops._mat_mul(\n\u001b[0;32m-> 1855\u001b[0;31m           a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_math_ops.pyc\u001b[0m in \u001b[0;36m_mat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   1452\u001b[0m   \"\"\"\n\u001b[1;32m   1453\u001b[0m   result = _op_def_lib.apply_op(\"MatMul\", a=a, b=b, transpose_a=transpose_a,\n\u001b[0;32m-> 1454\u001b[0;31m                                 transpose_b=transpose_b, name=name)\n\u001b[0m\u001b[1;32m   1455\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.pyc\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    761\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[1;32m    762\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m                          op_def=op_def)\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcreate_op\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\u001b[0m\n\u001b[1;32m   2395\u001b[0m                     original_op=self._default_original_op, op_def=op_def)\n\u001b[1;32m   2396\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompute_shapes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2397\u001b[0;31m       \u001b[0mset_shapes_for_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2398\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2399\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_op_seen_by_control_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mset_shapes_for_outputs\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1755\u001b[0m       \u001b[0mshape_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m   \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1758\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1759\u001b[0m     raise RuntimeError(\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.pyc\u001b[0m in \u001b[0;36mcall_with_requiring\u001b[0;34m(op)\u001b[0m\n\u001b[1;32m   1705\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1707\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcall_cpp_shape_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequire_shape_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0m_call_cpp_shape_fn_and_require_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_with_requiring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36mcall_cpp_shape_fn\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    608\u001b[0m     res = _call_cpp_shape_fn_impl(op, input_tensors_needed,\n\u001b[1;32m    609\u001b[0m                                   \u001b[0minput_tensors_as_shapes_needed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 610\u001b[0;31m                                   debug_python_shape_fn, require_shape_fn)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m       \u001b[0;31m# Handles the case where _call_cpp_shape_fn_impl calls unknown_shape(op).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/common_shapes.pyc\u001b[0m in \u001b[0;36m_call_cpp_shape_fn_impl\u001b[0;34m(op, input_tensors_needed, input_tensors_as_shapes_needed, debug_python_shape_fn, require_shape_fn)\u001b[0m\n\u001b[1;32m    673\u001b[0m       \u001b[0mmissing_shape_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mmissing_shape_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dimensions must be equal, but are 64 and 27 for 'MatMul_1' (op: 'MatMul') with input shapes: [64,64], [27,256]."
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # DECODER\n",
    "    def lstm():\n",
    "        shape = [vocabulary_size, num_nodes * 4]\n",
    "        W_i = tf.Variable(tf.truncated_normal(shape, -0.1, 0.1))\n",
    "        W_h = tf.Variable(tf.truncated_normal(shape, -0.1, 0.1))\n",
    "        b_i = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "        def cell(i, o, state):\n",
    "            cell = tf.matmul(i, W_i) + tf.matmul(o, W_h) + b_i\n",
    "            (input_gate, forget_gate, update, output_gate) = tf.split(cell, 4, axis=1)\n",
    "            input_gate = tf.sigmoid(input_gate)\n",
    "            forget_gate = tf.sigmoid(forget_gate)\n",
    "            output_gate = tf.sigmoid(output_gate)\n",
    "            state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "            return (output_gate * tf.tanh(state), state)\n",
    "        \n",
    "        return cell\n",
    "\n",
    "    # Input data.\n",
    "    encoder_train_inputs = []\n",
    "    decoder_train_inputs = []\n",
    "    train_labels = []\n",
    "    input_shape = [batch_size, vocabulary_size]\n",
    "    for _ in range(num_unrollings):\n",
    "        encoder_train_inputs.append(tf.placeholder(tf.float32, shape=input_shape))\n",
    "        decoder_train_inputs.append(tf.placeholder(tf.float32, shape=input_shape))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=input_shape))\n",
    "\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)  \n",
    "    # encoder LSTM loop.\n",
    "    encoder_state = saved_state\n",
    "    encoder_output = saved_output\n",
    "    encoder = lstm()\n",
    "    for encoder_input in encoder_train_inputs:\n",
    "        (encoder_output, encoder_state) = encoder(encoder_input,\n",
    "                                                  encoder_output,\n",
    "                                                  encoder_state)\n",
    "\n",
    "    # decoder LSTM loop.\n",
    "    decoder_state = encoder_state\n",
    "    decoder_output = encoder_output\n",
    "    decoder = lstm()\n",
    "    outputs = []\n",
    "    for decoder_input in decoder_train_inputs:\n",
    "        (decoder_output, decoder_state) = decoder(decoder_input,\n",
    "                                                  decoder_output,\n",
    "                                                  decoder_state)\n",
    "        outputs.append(decoder_output)\n",
    "\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        labels = tf.concat(train_labels, 0)\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels,\n",
    "                                                                logits=logits)\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step,\n",
    "                                               5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    (gradients, v) = zip(*optimizer.compute_gradients(loss))\n",
    "    (gradients, _) = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "    # Validation\n",
    "    sample_inputs = []\n",
    "    for _ in range(num_unrollings):\n",
    "        sample_inputs.append(tf.placeholder(tf.float32, shape=[1, vocabulary_size]))\n",
    "\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "    valid_output = saved_sample_output\n",
    "    valid_state = saved_sample_state\n",
    "    valid_encoder = lstm()\n",
    "    for valid_input in sample_inputs:\n",
    "        (valid_output, valid_state) = valid_encoder(valid_input,\n",
    "                                                    valid_output,\n",
    "                                                    valid_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-b216a012f64b>, line 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-b216a012f64b>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    decoder_input =\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    (x_batches, y_batches) = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings):\n",
    "      feed_dict[train_inputs[i]] = x_batches[i]\n",
    "      feed_dict[train_labels[i]] = y_batches[i]\n",
    "    \n",
    "    (_, l, predictions, lr) = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    \n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(y_batches)\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      \n",
    "      reset_sample_state.run()\n",
    "      (x_batches, y_batches) = valid_batches.next()\n",
    "      valid_logprob = 0\n",
    "      decoder_input = 0\n",
    "      for _ in range(num_unrollings):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
