{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    data = loadmat('ex8_movies.mat')\n",
    "    return (data['R'], data['Y'])\n",
    "\n",
    "def load_pretrained_params():\n",
    "    data = loadmat('ex8_movieParams.mat')\n",
    "    return (data['X'], data['Theta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.224603725685675"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collaborative_filtering():\n",
    "    (R, Y) = load_movie_data()\n",
    "    (X, Theta) = load_pretrained_params()\n",
    "    nm = 5\n",
    "    nu = 4\n",
    "    n = 100\n",
    "    nf = 3\n",
    "    X = X[:nm, :nf]\n",
    "    Theta = Theta[:nu, :nf]\n",
    "    Y = Y[:nm, :nu]\n",
    "    R = R[:nm, :nu]\n",
    "    \n",
    "    #cost = 0.0\n",
    "    #for (i, j) in itertools.product(range(nm), range(nu)):\n",
    "    #    if R[i, j] == 1:\n",
    "    #        cost += ((Theta[j, :].T.dot(X[i, :]) - Y[i, j]) ** 2)\n",
    "    squared_error = np.square(np.multiply(X.dot(Theta.T), R) - Y)\n",
    "    cost = np.sum(squared_error)\n",
    "        \n",
    "    return (1.0 / 2.0) * cost\n",
    "    \n",
    "collaborative_filtering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing numerical gradient...\n",
      "Computing J(...)...\n",
      "Diff 1.18742444121e-12\n"
     ]
    }
   ],
   "source": [
    "def J(X, R, Y, Theta):\n",
    "    error = np.multiply(X.dot(Theta.T), R) - Y\n",
    "    squared_error = np.square(error)\n",
    "    cost = np.sum(squared_error)\n",
    "    cost = (1.0 / 2.0) * cost\n",
    "    \n",
    "    d_dx = np.zeros(X.shape)\n",
    "    for i in range(len(X)):\n",
    "        x_i = X[i, :].reshape(1, -1)\n",
    "        idx = R[i, :] == 1\n",
    "        theta_t = Theta[idx, :]\n",
    "        m = x_i.dot(theta_t.T)\n",
    "        y_t = Y[i, idx].reshape(1, -1)\n",
    "        d_dx[i, :] = (m - y_t).dot(theta_t)\n",
    "        \n",
    "    d_dtheta = np.zeros(Theta.shape)\n",
    "    for j in range(len(Theta)):\n",
    "        theta_j = Theta[j, :].reshape(1, -1)\n",
    "        idx = R[:, j] == 1\n",
    "        x_t = X[idx, :]\n",
    "        m = theta_j.dot(x_t.T)\n",
    "        y_t = Y[idx, j].reshape(1, -1)\n",
    "        d_dtheta[j, :] = (m - y_t).dot(x_t)\n",
    "        \n",
    "    return (cost, d_dx, d_dtheta)\n",
    "\n",
    "def gradient_checking(X, R, Y, Theta, e=1e-4):\n",
    "    # compute Theta numerical gradient\n",
    "    s = Theta.shape\n",
    "    Theta_numgrad = np.zeros(s)\n",
    "    perturb = np.zeros(s)\n",
    "    indices = itertools.product(range(s[0]), range(s[1]))\n",
    "    for (i, j) in indices:\n",
    "        perturb[i, j] = e\n",
    "        (l1, _, _) = J(X, R, Y, Theta - perturb)\n",
    "        (l2, _, _) = J(X, R, Y, Theta + perturb)\n",
    "        Theta_numgrad[i, j] = (l2 - l1) / (2 * e)\n",
    "        perturb[i, j] = 0\n",
    "        \n",
    "    # compute Theta numerical gradient\n",
    "    s = X.shape\n",
    "    X_numgrad = np.zeros(s)\n",
    "    perturb = np.zeros(s)\n",
    "    indices = itertools.product(range(s[0]), range(s[1]))\n",
    "    for (i, j) in indices:\n",
    "        perturb[i, j] = e\n",
    "        (l1, _, _) = J(X - perturb, R, Y, Theta)\n",
    "        (l2, _, _) = J(X + perturb, R, Y, Theta)\n",
    "        X_numgrad[i, j] = (l2 - l1) / (2 * e)\n",
    "        perturb[i, j] = 0\n",
    "        \n",
    "    return (X_numgrad, Theta_numgrad)\n",
    "\n",
    "X_t = np.random.uniform(size=(4, 3))\n",
    "Theta_t = np.random.uniform(size=(5, 3))\n",
    "Y = X_t.dot(Theta_t.T)\n",
    "Y[np.random.uniform(size=Y.shape) > 0.5] = 0\n",
    "R = np.zeros(Y.shape)\n",
    "R[Y != 0] = 1\n",
    "X = np.random.normal(size=X_t.shape)\n",
    "Theta = np.random.normal(size=Theta_t.shape)\n",
    "\n",
    "print('Computing numerical gradient...')\n",
    "(X_numgrad, Theta_numgrad) = gradient_checking(X, R, Y, Theta)\n",
    "print('Computing J(...)...')\n",
    "(_, d_dx, d_dtheta) = J(X, R, Y, Theta)\n",
    "\n",
    "numgrad = np.vstack([X_numgrad, Theta_numgrad])\n",
    "grad = np.vstack([d_dx, d_dtheta])\n",
    "diff = np.linalg.norm(numgrad - grad) / np.linalg.norm(numgrad + grad)\n",
    "print('Diff', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1682, 10)\n",
      "Loss 31.3440562443\n",
      "Computing numerical gradient...\n",
      "Computing J(...)...\n",
      "Diff 2.30490196677e-12\n"
     ]
    }
   ],
   "source": [
    "def J(X, R, Y, Theta, l=1):\n",
    "    error = np.multiply(X.dot(Theta.T), R) - Y\n",
    "    squared_error = np.square(error)\n",
    "    cost = np.sum(squared_error)\n",
    "    theta_reg = (l / 2.0) * np.square(Theta).sum()\n",
    "    x_reg = (l / 2.0) * np.square(X).sum()\n",
    "    cost = (1.0 / 2.0) * cost + theta_reg + x_reg\n",
    "    \n",
    "    d_dx = np.zeros(X.shape)\n",
    "    for i in range(len(X)):\n",
    "        x_i = X[i, :].reshape(1, -1)\n",
    "        idx = R[i, :] == 1\n",
    "        theta_t = Theta[idx, :]\n",
    "        m = x_i.dot(theta_t.T)\n",
    "        y_t = Y[i, idx].reshape(1, -1)\n",
    "        d_dx[i, :] = (m - y_t).dot(theta_t) + (l * x_i)\n",
    "        \n",
    "    d_dtheta = np.zeros(Theta.shape)\n",
    "    for j in range(len(Theta)):\n",
    "        theta_j = Theta[j, :].reshape(1, -1)\n",
    "        idx = R[:, j] == 1\n",
    "        x_t = X[idx, :]\n",
    "        m = theta_j.dot(x_t.T)\n",
    "        y_t = Y[idx, j].reshape(1, -1)\n",
    "        d_dtheta[j, :] = (m - y_t).dot(x_t) + (l * theta_j)\n",
    "        \n",
    "    return (cost, d_dx, d_dtheta)\n",
    "\n",
    "def data_subset(nm, nu, nf):\n",
    "    (R, Y) = load_movie_data()\n",
    "    (X, Theta) = load_pretrained_params()\n",
    "    X = X[:nm, :nf]\n",
    "    Theta = Theta[:nu, :nf]\n",
    "    Y = Y[:nm, :nu]\n",
    "    R = R[:nm, :nu]\n",
    "\n",
    "    return (X, R, Y, Theta)\n",
    "\n",
    "(X, R, Y, Theta) = data_subset(nm=5, nu=4, nf=3)\n",
    "(loss, _, _) = J(X, R, Y, Theta, l=1.5)\n",
    "print('Loss', loss)\n",
    "\n",
    "X_t = np.random.uniform(size=(4, 3))\n",
    "Theta_t = np.random.uniform(size=(5, 3))\n",
    "Y = X_t.dot(Theta_t.T)\n",
    "Y[np.random.uniform(size=Y.shape) > 0.5] = 0\n",
    "R = np.zeros(Y.shape)\n",
    "R[Y != 0] = 1\n",
    "X = np.random.normal(size=X_t.shape)\n",
    "Theta = np.random.normal(size=Theta_t.shape)\n",
    "\n",
    "print('Computing numerical gradient...')\n",
    "(X_numgrad, Theta_numgrad) = gradient_checking(X, R, Y, Theta)\n",
    "print('Computing J(...)...')\n",
    "(_, d_dx, d_dtheta) = J(X, R, Y, Theta)\n",
    "\n",
    "numgrad = np.vstack([X_numgrad, Theta_numgrad])\n",
    "grad = np.vstack([d_dx, d_dtheta])\n",
    "diff = np.linalg.norm(numgrad - grad) / np.linalg.norm(numgrad + grad)\n",
    "print('Diff', diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_ratings(Y, R):\n",
    "    means = np.zeros(len(Y))\n",
    "    norms = np.zeros(Y.shape)\n",
    "    for i in range(len(Y)):\n",
    "        idx = R[i, :] == 1\n",
    "        means[i] = np.mean(Y[i, idx])\n",
    "        norms[i, idx] = Y[i, idx] - means[i]\n",
    "        \n",
    "    return (norms, means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at  0 676862.19729\n",
      "Loss at  1 369474.561025\n",
      "Loss at  2 278173.221767\n",
      "Loss at  3 231999.9781\n",
      "Loss at  4 203766.933012\n",
      "Loss at  5 184556.834868\n",
      "Loss at  6 170537.989328\n",
      "Loss at  7 159784.219006\n",
      "Loss at  8 151219.891336\n",
      "Loss at  9 144196.794386\n",
      "Loss at  10 138300.938309\n",
      "Loss at  11 133255.506439\n",
      "Loss at  12 128868.375742\n",
      "Loss at  13 125002.016947\n",
      "Loss at  14 121555.377135\n",
      "Loss at  15 118452.525838\n",
      "Loss at  16 115635.291747\n",
      "Loss at  17 113058.344539\n",
      "Loss at  18 110685.824477\n",
      "Loss at  19 108488.980031\n",
      "Loss at  20 106444.478618\n",
      "Loss at  21 104533.176946\n",
      "Loss at  22 102739.211459\n",
      "Loss at  23 101049.315759\n",
      "Loss at  24 99452.3015783\n",
      "Loss at  25 97938.6593752\n",
      "Loss at  26 96500.2475694\n",
      "Loss at  27 95130.0483096\n",
      "Loss at  28 93821.973727\n",
      "Loss at  29 92570.7109119\n",
      "Loss at  30 91371.5968809\n",
      "Loss at  31 90220.5169846\n",
      "Loss at  32 89113.8217902\n",
      "Loss at  33 88048.2586471\n",
      "Loss at  34 87020.915006\n",
      "Loss at  35 86029.1712191\n",
      "Loss at  36 85070.6610386\n",
      "Loss at  37 84143.2384084\n",
      "Loss at  38 83244.9494372\n",
      "Loss at  39 82374.00866\n",
      "Loss at  40 81528.7788767\n",
      "Loss at  41 80707.7539897\n",
      "Loss at  42 79909.544375\n",
      "Loss at  43 79132.8644028\n",
      "Loss at  44 78376.5217986\n",
      "Loss at  45 77639.4085833\n",
      "Loss at  46 76920.49338\n",
      "Loss at  47 76218.8149056\n",
      "Loss at  48 75533.4764948\n",
      "Loss at  49 74863.6415221\n",
      "Loss at  50 74208.5296061\n",
      "Loss at  51 73567.413489\n",
      "Loss at  52 72939.6164944\n",
      "Loss at  53 72324.5104698\n",
      "Loss at  54 71721.5141239\n",
      "Loss at  55 71130.091671\n",
      "Loss at  56 70549.751693\n",
      "Loss at  57 69980.0461337\n",
      "Loss at  58 69420.5693359\n",
      "Loss at  59 68870.9570404\n",
      "Loss at  60 68330.8852641\n",
      "Loss at  61 67800.0689866\n",
      "Loss at  62 67278.2605812\n",
      "Loss at  63 66765.2479414\n",
      "Loss at  64 66260.8522685\n",
      "Loss at  65 65764.9255075\n",
      "Loss at  66 65277.3474373\n",
      "Loss at  67 64798.0224443\n",
      "Loss at  68 64326.8760296\n",
      "Loss at  69 63863.8511201\n",
      "Loss at  70 63408.9042689\n",
      "Loss at  71 62962.0018439\n",
      "Loss at  72 62523.116306\n",
      "Loss at  73 62092.2226804\n",
      "Loss at  74 61669.2953151\n",
      "Loss at  75 61254.3050097\n",
      "Loss at  76 60847.2165769\n",
      "Loss at  77 60447.986881\n",
      "Loss at  78 60056.5633727\n",
      "Loss at  79 59672.8831165\n",
      "Loss at  80 59296.8722894\n",
      "Loss at  81 58928.4461072\n",
      "Loss at  82 58567.5091266\n",
      "Loss at  83 58213.9558566\n",
      "Loss at  84 57867.6716149\n",
      "Loss at  85 57528.5335591\n",
      "Loss at  86 57196.4118312\n",
      "Loss at  87 56871.1707554\n",
      "Loss at  88 56552.6700436\n",
      "Loss at  89 56240.7659651\n",
      "Loss at  90 55935.3124527\n",
      "Loss at  91 55636.1621221\n",
      "Loss at  92 55343.1671918\n",
      "Loss at  93 55056.1802958\n",
      "Loss at  94 54775.0551892\n",
      "Loss at  95 54499.6473484\n",
      "Loss at  96 54229.8144727\n",
      "Loss at  97 53965.4168948\n",
      "Loss at  98 53706.3179102\n",
      "Loss at  99 53452.3840339\n",
      "Loss at  100 53203.4851943\n",
      "Loss at  101 52959.4948731\n",
      "Loss at  102 52720.2901998\n",
      "Loss at  103 52485.7520071\n",
      "Loss at  104 52255.7648547\n",
      "Loss at  105 52030.2170261\n",
      "Loss at  106 51809.0005035\n",
      "Loss at  107 51592.0109252\n",
      "Loss at  108 51379.1475277\n",
      "Loss at  109 51170.3130755\n",
      "Loss at  110 50965.4137818\n",
      "Loss at  111 50764.3592205\n",
      "Loss at  112 50567.0622313\n",
      "Loss at  113 50373.4388197\n",
      "Loss at  114 50183.4080525\n",
      "Loss at  115 49996.8919495\n",
      "Loss at  116 49813.8153717\n",
      "Loss at  117 49634.1059084\n",
      "Loss at  118 49457.6937614\n",
      "Loss at  119 49284.5116288\n",
      "Loss at  120 49114.4945875\n",
      "Loss at  121 48947.5799765\n",
      "Loss at  122 48783.7072796\n",
      "Loss at  123 48622.8180097\n",
      "Loss at  124 48464.855594\n",
      "Loss at  125 48309.7652618\n",
      "Loss at  126 48157.4939338\n",
      "Loss at  127 48007.9901146\n",
      "Loss at  128 47861.203789\n",
      "Loss at  129 47717.0863207\n",
      "Loss at  130 47575.5903567\n",
      "Loss at  131 47436.6697347\n",
      "Loss at  132 47300.2793958\n",
      "Loss at  133 47166.3753023\n",
      "Loss at  134 47034.9143607\n",
      "Loss at  135 46905.854349\n",
      "Loss at  136 46779.1538512\n",
      "Loss at  137 46654.7721954\n",
      "Loss at  138 46532.669399\n",
      "Loss at  139 46412.8061179\n",
      "Loss at  140 46295.1436019\n",
      "Loss at  141 46179.6436553\n",
      "Loss at  142 46066.2686013\n",
      "Loss at  143 45954.9812529\n",
      "Loss at  144 45845.7448869\n",
      "Loss at  145 45738.5232226\n",
      "Loss at  146 45633.2804046\n",
      "Loss at  147 45529.9809891\n",
      "Loss at  148 45428.5899333\n",
      "Loss at  149 45329.072588\n",
      "Loss at  150 45231.3946927\n",
      "Loss at  151 45135.5223735\n",
      "Loss at  152 45041.4221421\n",
      "Loss at  153 44949.0608976\n",
      "Loss at  154 44858.4059293\n",
      "Loss at  155 44769.4249202\n",
      "Loss at  156 44682.0859521\n",
      "Loss at  157 44596.3575115\n",
      "Loss at  158 44512.2084952\n",
      "Loss at  159 44429.608217\n",
      "Loss at  160 44348.5264139\n",
      "Loss at  161 44268.9332529\n",
      "Loss at  162 44190.799337\n",
      "Loss at  163 44114.0957113\n",
      "Loss at  164 44038.7938685\n",
      "Loss at  165 43964.8657543\n",
      "Loss at  166 43892.2837719\n",
      "Loss at  167 43821.0207856\n",
      "Loss at  168 43751.0501249\n",
      "Loss at  169 43682.3455866\n",
      "Loss at  170 43614.8814374\n",
      "Loss at  171 43548.6324145\n",
      "Loss at  172 43483.5737269\n",
      "Loss at  173 43419.6810553\n",
      "Loss at  174 43356.930551\n",
      "Loss at  175 43295.2988349\n",
      "Loss at  176 43234.7629955\n",
      "Loss at  177 43175.3005861\n",
      "Loss at  178 43116.8896219\n",
      "Loss at  179 43059.5085761\n",
      "Loss at  180 43003.1363758\n",
      "Loss at  181 42947.7523974\n",
      "Loss at  182 42893.336461\n",
      "Loss at  183 42839.8688256\n",
      "Loss at  184 42787.3301827\n",
      "Loss at  185 42735.7016501\n",
      "Loss at  186 42684.9647657\n",
      "Loss at  187 42635.1014802\n",
      "Loss at  188 42586.0941506\n",
      "Loss at  189 42537.9255327\n",
      "Loss at  190 42490.5787738\n",
      "Loss at  191 42444.0374049\n",
      "Loss at  192 42398.2853337\n",
      "Loss at  193 42353.3068363\n",
      "Loss at  194 42309.0865495\n",
      "Loss at  195 42265.6094634\n",
      "Loss at  196 42222.860913\n",
      "Loss at  197 42180.8265709\n",
      "Loss at  198 42139.492439\n",
      "Loss at  199 42098.8448412\n",
      "Loss at  200 42058.8704155\n",
      "Loss at  201 42019.5561063\n",
      "Loss at  202 41980.8891569\n",
      "Loss at  203 41942.8571023\n",
      "Loss at  204 41905.4477612\n",
      "Loss at  205 41868.6492298\n",
      "Loss at  206 41832.4498735\n",
      "Loss at  207 41796.838321\n",
      "Loss at  208 41761.8034567\n",
      "Loss at  209 41727.3344145\n",
      "Loss at  210 41693.4205707\n",
      "Loss at  211 41660.0515383\n",
      "Loss at  212 41627.2171601\n",
      "Loss at  213 41594.907503\n",
      "Loss at  214 41563.1128517\n",
      "Loss at  215 41531.8237034\n",
      "Loss at  216 41501.0307616\n",
      "Loss at  217 41470.724931\n",
      "Loss at  218 41440.897312\n",
      "Loss at  219 41411.5391954\n",
      "Loss at  220 41382.6420577\n",
      "Loss at  221 41354.1975559\n",
      "Loss at  222 41326.1975228\n",
      "Loss at  223 41298.6339625\n",
      "Loss at  224 41271.4990458\n",
      "Loss at  225 41244.7851062\n",
      "Loss at  226 41218.4846351\n",
      "Loss at  227 41192.5902782\n",
      "Loss at  228 41167.0948315\n",
      "Loss at  229 41141.9912375\n",
      "Loss at  230 41117.2725811\n",
      "Loss at  231 41092.9320867\n",
      "Loss at  232 41068.9631141\n",
      "Loss at  233 41045.3591556\n",
      "Loss at  234 41022.1138326\n",
      "Loss at  235 40999.2208922\n",
      "Loss at  236 40976.6742045\n",
      "Loss at  237 40954.4677596\n",
      "Loss at  238 40932.5956645\n",
      "Loss at  239 40911.0521405\n",
      "Loss at  240 40889.8315207\n",
      "Loss at  241 40868.9282469\n",
      "Loss at  242 40848.3368677\n",
      "Loss at  243 40828.0520354\n",
      "Loss at  244 40808.0685042\n",
      "Loss at  245 40788.3811277\n",
      "Loss at  246 40768.9848564\n",
      "Loss at  247 40749.874736\n",
      "Loss at  248 40731.045905\n",
      "Loss at  249 40712.4935927\n",
      "Loss at  250 40694.2131174\n",
      "Loss at  251 40676.1998842\n",
      "Loss at  252 40658.4493834\n",
      "Loss at  253 40640.9571884\n",
      "Loss at  254 40623.7189543\n",
      "Loss at  255 40606.7304158\n",
      "Loss at  256 40589.9873858\n",
      "Loss at  257 40573.4857539\n",
      "Loss at  258 40557.2214842\n",
      "Loss at  259 40541.1906145\n",
      "Loss at  260 40525.3892544\n",
      "Loss at  261 40509.8135838\n",
      "Loss at  262 40494.4598519\n",
      "Loss at  263 40479.3243752\n",
      "Loss at  264 40464.4035367\n",
      "Loss at  265 40449.6937842\n",
      "Loss at  266 40435.1916292\n",
      "Loss at  267 40420.8936458\n",
      "Loss at  268 40406.796469\n",
      "Loss at  269 40392.896794\n",
      "Loss at  270 40379.1913748\n",
      "Loss at  271 40365.6770231\n",
      "Loss at  272 40352.350607\n",
      "Loss at  273 40339.2090505\n",
      "Loss at  274 40326.2493316\n",
      "Loss at  275 40313.4684821\n",
      "Loss at  276 40300.8635858\n",
      "Loss at  277 40288.4317782\n",
      "Loss at  278 40276.1702451\n",
      "Loss at  279 40264.0762218\n",
      "Loss at  280 40252.146992\n",
      "Loss at  281 40240.3798873\n",
      "Loss at  282 40228.7722858\n",
      "Loss at  283 40217.3216117\n",
      "Loss at  284 40206.0253341\n",
      "Loss at  285 40194.8809663\n",
      "Loss at  286 40183.8860649\n",
      "Loss at  287 40173.0382293\n",
      "Loss at  288 40162.3351005\n",
      "Loss at  289 40151.7743605\n",
      "Loss at  290 40141.3537317\n",
      "Loss at  291 40131.0709759\n",
      "Loss at  292 40120.9238939\n",
      "Loss at  293 40110.9103245\n",
      "Loss at  294 40101.0281438\n",
      "Loss at  295 40091.2752648\n",
      "Loss at  296 40081.6496364\n",
      "Loss at  297 40072.149243\n",
      "Loss at  298 40062.7721039\n",
      "Loss at  299 40053.5162723\n",
      "Loss at  300 40044.3798351\n",
      "Loss at  301 40035.3609119\n",
      "Loss at  302 40026.4576549\n",
      "Loss at  303 40017.6682479\n",
      "Loss at  304 40008.9909058\n",
      "Loss at  305 40000.4238743\n",
      "Loss at  306 39991.965429\n",
      "Loss at  307 39983.6138751\n",
      "Loss at  308 39975.3675469\n",
      "Loss at  309 39967.2248069\n",
      "Loss at  310 39959.1840459\n",
      "Loss at  311 39951.243682\n",
      "Loss at  312 39943.4021602\n",
      "Loss at  313 39935.6579521\n",
      "Loss at  314 39928.0095555\n",
      "Loss at  315 39920.4554934\n",
      "Loss at  316 39912.9943143\n",
      "Loss at  317 39905.624591\n",
      "Loss at  318 39898.3449209\n",
      "Loss at  319 39891.1539249\n",
      "Loss at  320 39884.0502474\n",
      "Loss at  321 39877.0325558\n",
      "Loss at  322 39870.09954\n",
      "Loss at  323 39863.2499121\n",
      "Loss at  324 39856.4824058\n",
      "Loss at  325 39849.7957766\n",
      "Loss at  326 39843.1888005\n",
      "Loss at  327 39836.6602744\n",
      "Loss at  328 39830.2090155\n",
      "Loss at  329 39823.8338609\n",
      "Loss at  330 39817.5336672\n",
      "Loss at  331 39811.3073101\n",
      "Loss at  332 39805.1536845\n",
      "Loss at  333 39799.0717036\n",
      "Loss at  334 39793.0602989\n",
      "Loss at  335 39787.1184198\n",
      "Loss at  336 39781.2450332\n",
      "Loss at  337 39775.4391234\n",
      "Loss at  338 39769.6996918\n",
      "Loss at  339 39764.0257561\n",
      "Loss at  340 39758.4163507\n",
      "Loss at  341 39752.8705261\n",
      "Loss at  342 39747.3873483\n",
      "Loss at  343 39741.9658993\n",
      "Loss at  344 39736.605276\n",
      "Loss at  345 39731.3045904\n",
      "Loss at  346 39726.0629694\n",
      "Loss at  347 39720.8795541\n",
      "Loss at  348 39715.7535002\n",
      "Loss at  349 39710.683977\n",
      "Loss at  350 39705.6701677\n",
      "Loss at  351 39700.711269\n",
      "Loss at  352 39695.806491\n",
      "Loss at  353 39690.9550565\n",
      "Loss at  354 39686.1562014\n",
      "Loss at  355 39681.409174\n",
      "Loss at  356 39676.7132352\n",
      "Loss at  357 39672.0676576\n",
      "Loss at  358 39667.4717263\n",
      "Loss at  359 39662.9247376\n",
      "Loss at  360 39658.4259997\n",
      "Loss at  361 39653.974832\n",
      "Loss at  362 39649.5705649\n",
      "Loss at  363 39645.2125399\n",
      "Loss at  364 39640.9001092\n",
      "Loss at  365 39636.6326355\n",
      "Loss at  366 39632.4094919\n",
      "Loss at  367 39628.2300618\n",
      "Loss at  368 39624.0937385\n",
      "Loss at  369 39619.999925\n",
      "Loss at  370 39615.9480344\n",
      "Loss at  371 39611.9374888\n",
      "Loss at  372 39607.9677201\n",
      "Loss at  373 39604.038169\n",
      "Loss at  374 39600.1482855\n",
      "Loss at  375 39596.2975282\n",
      "Loss at  376 39592.4853648\n",
      "Loss at  377 39588.7112711\n",
      "Loss at  378 39584.9747316\n",
      "Loss at  379 39581.275239\n",
      "Loss at  380 39577.6122941\n",
      "Loss at  381 39573.9854058\n",
      "Loss at  382 39570.3940908\n",
      "Loss at  383 39566.8378733\n",
      "Loss at  384 39563.3162853\n",
      "Loss at  385 39559.8288663\n",
      "Loss at  386 39556.3751629\n",
      "Loss at  387 39552.9547292\n",
      "Loss at  388 39549.567126\n",
      "Loss at  389 39546.2119213\n",
      "Loss at  390 39542.8886899\n",
      "Loss at  391 39539.5970132\n",
      "Loss at  392 39536.3364794\n",
      "Loss at  393 39533.1066831\n",
      "Loss at  394 39529.9072251\n",
      "Loss at  395 39526.7377127\n",
      "Loss at  396 39523.5977592\n",
      "Loss at  397 39520.486984\n",
      "Loss at  398 39517.4050126\n",
      "Loss at  399 39514.3514762\n",
      "Loss at  400 39511.3260116\n",
      "Loss at  401 39508.3282616\n",
      "Loss at  402 39505.3578742\n",
      "Loss at  403 39502.4145031\n",
      "Loss at  404 39499.4978074\n",
      "Loss at  405 39496.6074512\n",
      "Loss at  406 39493.743104\n",
      "Loss at  407 39490.9044403\n",
      "Loss at  408 39488.0911397\n",
      "Loss at  409 39485.3028867\n",
      "Loss at  410 39482.5393706\n",
      "Loss at  411 39479.8002854\n",
      "Loss at  412 39477.08533\n",
      "Loss at  413 39474.3942076\n",
      "Loss at  414 39471.7266262\n",
      "Loss at  415 39469.082298\n",
      "Loss at  416 39466.4609397\n",
      "Loss at  417 39463.8622723\n",
      "Loss at  418 39461.2860211\n",
      "Loss at  419 39458.7319153\n",
      "Loss at  420 39456.1996884\n",
      "Loss at  421 39453.6890778\n",
      "Loss at  422 39451.1998249\n",
      "Loss at  423 39448.731675\n",
      "Loss at  424 39446.2843772\n",
      "Loss at  425 39443.8576843\n",
      "Loss at  426 39441.4513527\n",
      "Loss at  427 39439.0651427\n",
      "Loss at  428 39436.698818\n",
      "Loss at  429 39434.3521456\n",
      "Loss at  430 39432.0248963\n",
      "Loss at  431 39429.7168442\n",
      "Loss at  432 39427.4277665\n",
      "Loss at  433 39425.1574439\n",
      "Loss at  434 39422.9056602\n",
      "Loss at  435 39420.6722026\n",
      "Loss at  436 39418.456861\n",
      "Loss at  437 39416.2594288\n",
      "Loss at  438 39414.079702\n",
      "Loss at  439 39411.91748\n",
      "Loss at  440 39409.7725647\n",
      "Loss at  441 39407.644761\n",
      "Loss at  442 39405.5338769\n",
      "Loss at  443 39403.4397227\n",
      "Loss at  444 39401.3621116\n",
      "Loss at  445 39399.3008598\n",
      "Loss at  446 39397.2557856\n",
      "Loss at  447 39395.2267102\n",
      "Loss at  448 39393.2134573\n",
      "Loss at  449 39391.2158532\n",
      "Loss at  450 39389.2337264\n",
      "Loss at  451 39387.2669081\n",
      "Loss at  452 39385.3152317\n",
      "Loss at  453 39383.378533\n",
      "Loss at  454 39381.4566501\n",
      "Loss at  455 39379.5494235\n",
      "Loss at  456 39377.6566957\n",
      "Loss at  457 39375.7783115\n",
      "Loss at  458 39373.914118\n",
      "Loss at  459 39372.0639641\n",
      "Loss at  460 39370.2277011\n",
      "Loss at  461 39368.4051823\n",
      "Loss at  462 39366.5962627\n",
      "Loss at  463 39364.8007998\n",
      "Loss at  464 39363.0186527\n",
      "Loss at  465 39361.2496824\n",
      "Loss at  466 39359.4937521\n",
      "Loss at  467 39357.7507266\n",
      "Loss at  468 39356.0204726\n",
      "Loss at  469 39354.3028587\n",
      "Loss at  470 39352.597755\n",
      "Loss at  471 39350.9050337\n",
      "Loss at  472 39349.2245684\n",
      "Loss at  473 39347.5562347\n",
      "Loss at  474 39345.8999096\n",
      "Loss at  475 39344.2554718\n",
      "Loss at  476 39342.6228018\n",
      "Loss at  477 39341.0017814\n",
      "Loss at  478 39339.3922941\n",
      "Loss at  479 39337.7942249\n",
      "Loss at  480 39336.2074604\n",
      "Loss at  481 39334.6318886\n",
      "Loss at  482 39333.067399\n",
      "Loss at  483 39331.5138825\n",
      "Loss at  484 39329.9712313\n",
      "Loss at  485 39328.4393394\n",
      "Loss at  486 39326.9181016\n",
      "Loss at  487 39325.4074146\n",
      "Loss at  488 39323.9071761\n",
      "Loss at  489 39322.4172851\n",
      "Loss at  490 39320.9376421\n",
      "Loss at  491 39319.4681487\n",
      "Loss at  492 39318.0087077\n",
      "Loss at  493 39316.5592234\n",
      "Loss at  494 39315.119601\n",
      "Loss at  495 39313.689747\n",
      "Loss at  496 39312.2695692\n",
      "Loss at  497 39310.8589764\n",
      "Loss at  498 39309.4578785\n",
      "Loss at  499 39308.0661868\n",
      "Loss at  500 39306.6838133\n",
      "Loss at  501 39305.3106714\n",
      "Loss at  502 39303.9466754\n",
      "Loss at  503 39302.5917408\n",
      "Loss at  504 39301.2457839\n",
      "Loss at  505 39299.9087223\n",
      "Loss at  506 39298.5804743\n",
      "Loss at  507 39297.2609596\n",
      "Loss at  508 39295.9500984\n",
      "Loss at  509 39294.6478123\n",
      "Loss at  510 39293.3540234\n",
      "Loss at  511 39292.0686551\n",
      "Loss at  512 39290.7916317\n",
      "Loss at  513 39289.522878\n",
      "Loss at  514 39288.2623202\n",
      "Loss at  515 39287.0098851\n",
      "Loss at  516 39285.7655004\n",
      "Loss at  517 39284.5290946\n",
      "Loss at  518 39283.3005972\n",
      "Loss at  519 39282.0799384\n",
      "Loss at  520 39280.8670492\n",
      "Loss at  521 39279.6618615\n",
      "Loss at  522 39278.4643079\n",
      "Loss at  523 39277.2743218\n",
      "Loss at  524 39276.0918374\n",
      "Loss at  525 39274.9167896\n",
      "Loss at  526 39273.7491142\n",
      "Loss at  527 39272.5887474\n",
      "Loss at  528 39271.4356265\n",
      "Loss at  529 39270.2896892\n",
      "Loss at  530 39269.1508742\n",
      "Loss at  531 39268.0191207\n",
      "Loss at  532 39266.8943685\n",
      "Loss at  533 39265.7765583\n",
      "Loss at  534 39264.6656314\n",
      "Loss at  535 39263.5615296\n",
      "Loss at  536 39262.4641955\n",
      "Loss at  537 39261.3735723\n",
      "Loss at  538 39260.2896038\n",
      "Loss at  539 39259.2122344\n",
      "Loss at  540 39258.1414092\n",
      "Loss at  541 39257.0770738\n",
      "Loss at  542 39256.0191745\n",
      "Loss at  543 39254.967658\n",
      "Loss at  544 39253.9224718\n",
      "Loss at  545 39252.8835637\n",
      "Loss at  546 39251.8508824\n",
      "Loss at  547 39250.8243769\n",
      "Loss at  548 39249.8039968\n",
      "Loss at  549 39248.7896923\n",
      "Loss at  550 39247.781414\n",
      "Loss at  551 39246.7791131\n",
      "Loss at  552 39245.7827414\n",
      "Loss at  553 39244.792251\n",
      "Loss at  554 39243.8075948\n",
      "Loss at  555 39242.8287259\n",
      "Loss at  556 39241.855598\n",
      "Loss at  557 39240.8881653\n",
      "Loss at  558 39239.9263825\n",
      "Loss at  559 39238.9702047\n",
      "Loss at  560 39238.0195876\n",
      "Loss at  561 39237.0744871\n",
      "Loss at  562 39236.1348598\n",
      "Loss at  563 39235.2006626\n",
      "Loss at  564 39234.2718528\n",
      "Loss at  565 39233.3483885\n",
      "Loss at  566 39232.4302276\n",
      "Loss at  567 39231.517329\n",
      "Loss at  568 39230.6096517\n",
      "Loss at  569 39229.7071552\n",
      "Loss at  570 39228.8097994\n",
      "Loss at  571 39227.9175446\n",
      "Loss at  572 39227.0303515\n",
      "Loss at  573 39226.1481812\n",
      "Loss at  574 39225.2709951\n",
      "Loss at  575 39224.3987551\n",
      "Loss at  576 39223.5314235\n",
      "Loss at  577 39222.6689627\n",
      "Loss at  578 39221.8113359\n",
      "Loss at  579 39220.9585062\n",
      "Loss at  580 39220.1104374\n",
      "Loss at  581 39219.2670936\n",
      "Loss at  582 39218.428439\n",
      "Loss at  583 39217.5944385\n",
      "Loss at  584 39216.765057\n",
      "Loss at  585 39215.9402601\n",
      "Loss at  586 39215.1200133\n",
      "Loss at  587 39214.3042829\n",
      "Loss at  588 39213.4930351\n",
      "Loss at  589 39212.6862367\n",
      "Loss at  590 39211.8838546\n",
      "Loss at  591 39211.0858563\n",
      "Loss at  592 39210.2922093\n",
      "Loss at  593 39209.5028816\n",
      "Loss at  594 39208.7178414\n",
      "Loss at  595 39207.9370573\n",
      "Loss at  596 39207.1604981\n",
      "Loss at  597 39206.3881329\n",
      "Loss at  598 39205.6199312\n",
      "Loss at  599 39204.8558626\n",
      "Loss at  600 39204.0958971\n",
      "Loss at  601 39203.3400049\n",
      "Loss at  602 39202.5881567\n",
      "Loss at  603 39201.8403231\n",
      "Loss at  604 39201.0964752\n",
      "Loss at  605 39200.3565844\n",
      "Loss at  606 39199.6206222\n",
      "Loss at  607 39198.8885605\n",
      "Loss at  608 39198.1603714\n",
      "Loss at  609 39197.4360272\n",
      "Loss at  610 39196.7155005\n",
      "Loss at  611 39195.9987643\n",
      "Loss at  612 39195.2857915\n",
      "Loss at  613 39194.5765555\n",
      "Loss at  614 39193.87103\n",
      "Loss at  615 39193.1691885\n",
      "Loss at  616 39192.4710054\n",
      "Loss at  617 39191.7764547\n",
      "Loss at  618 39191.085511\n",
      "Loss at  619 39190.398149\n",
      "Loss at  620 39189.7143437\n",
      "Loss at  621 39189.0340702\n",
      "Loss at  622 39188.3573039\n",
      "Loss at  623 39187.6840205\n",
      "Loss at  624 39187.0141957\n",
      "Loss at  625 39186.3478055\n",
      "Loss at  626 39185.6848262\n",
      "Loss at  627 39185.0252342\n",
      "Loss at  628 39184.3690062\n",
      "Loss at  629 39183.716119\n",
      "Loss at  630 39183.0665496\n",
      "Loss at  631 39182.4202754\n",
      "Loss at  632 39181.7772737\n",
      "Loss at  633 39181.1375221\n",
      "Loss at  634 39180.5009986\n",
      "Loss at  635 39179.8676811\n",
      "Loss at  636 39179.2375477\n",
      "Loss at  637 39178.610577\n",
      "Loss at  638 39177.9867474\n",
      "Loss at  639 39177.3660378\n",
      "Loss at  640 39176.748427\n",
      "Loss at  641 39176.1338941\n",
      "Loss at  642 39175.5224185\n",
      "Loss at  643 39174.9139796\n",
      "Loss at  644 39174.308557\n",
      "Loss at  645 39173.7061306\n",
      "Loss at  646 39173.1066802\n",
      "Loss at  647 39172.5101861\n",
      "Loss at  648 39171.9166285\n",
      "Loss at  649 39171.325988\n",
      "Loss at  650 39170.738245\n",
      "Loss at  651 39170.1533806\n",
      "Loss at  652 39169.5713754\n",
      "Loss at  653 39168.9922108\n",
      "Loss at  654 39168.4158679\n",
      "Loss at  655 39167.8423282\n",
      "Loss at  656 39167.2715732\n",
      "Loss at  657 39166.7035847\n",
      "Loss at  658 39166.1383444\n",
      "Loss at  659 39165.5758345\n",
      "Loss at  660 39165.0160371\n",
      "Loss at  661 39164.4589345\n",
      "Loss at  662 39163.9045092\n",
      "Loss at  663 39163.3527437\n",
      "Loss at  664 39162.8036207\n",
      "Loss at  665 39162.2571233\n",
      "Loss at  666 39161.7132343\n",
      "Loss at  667 39161.1719369\n",
      "Loss at  668 39160.6332145\n",
      "Loss at  669 39160.0970503\n",
      "Loss at  670 39159.563428\n",
      "Loss at  671 39159.0323313\n",
      "Loss at  672 39158.503744\n",
      "Loss at  673 39157.9776499\n",
      "Loss at  674 39157.4540333\n",
      "Loss at  675 39156.9328782\n",
      "Loss at  676 39156.4141691\n",
      "Loss at  677 39155.8978902\n",
      "Loss at  678 39155.3840263\n",
      "Loss at  679 39154.872562\n",
      "Loss at  680 39154.3634822\n",
      "Loss at  681 39153.8567716\n",
      "Loss at  682 39153.3524155\n",
      "Loss at  683 39152.8503989\n",
      "Loss at  684 39152.3507072\n",
      "Loss at  685 39151.8533258\n",
      "Loss at  686 39151.35824\n",
      "Loss at  687 39150.8654357\n",
      "Loss at  688 39150.3748985\n",
      "Loss at  689 39149.8866142\n",
      "Loss at  690 39149.4005689\n",
      "Loss at  691 39148.9167486\n",
      "Loss at  692 39148.4351394\n",
      "Loss at  693 39147.9557276\n",
      "Loss at  694 39147.4784997\n",
      "Loss at  695 39147.0034421\n",
      "Loss at  696 39146.5305414\n",
      "Loss at  697 39146.0597843\n",
      "Loss at  698 39145.5911577\n",
      "Loss at  699 39145.1246483\n",
      "Loss at  700 39144.6602432\n",
      "Loss at  701 39144.1979296\n",
      "Loss at  702 39143.7376945\n",
      "Loss at  703 39143.2795254\n",
      "Loss at  704 39142.8234095\n",
      "Loss at  705 39142.3693345\n",
      "Loss at  706 39141.9172877\n",
      "Loss at  707 39141.467257\n",
      "Loss at  708 39141.0192302\n",
      "Loss at  709 39140.5731949\n",
      "Loss at  710 39140.1291393\n",
      "Loss at  711 39139.6870514\n",
      "Loss at  712 39139.2469192\n",
      "Loss at  713 39138.8087311\n",
      "Loss at  714 39138.3724753\n",
      "Loss at  715 39137.9381402\n",
      "Loss at  716 39137.5057143\n",
      "Loss at  717 39137.0751862\n",
      "Loss at  718 39136.6465445\n",
      "Loss at  719 39136.2197779\n",
      "Loss at  720 39135.7948754\n",
      "Loss at  721 39135.3718257\n",
      "Loss at  722 39134.9506179\n",
      "Loss at  723 39134.5312411\n",
      "Loss at  724 39134.1136843\n",
      "Loss at  725 39133.6979369\n",
      "Loss at  726 39133.2839881\n",
      "Loss at  727 39132.8718273\n",
      "Loss at  728 39132.4614441\n",
      "Loss at  729 39132.0528278\n",
      "Loss at  730 39131.6459682\n",
      "Loss at  731 39131.2408549\n",
      "Loss at  732 39130.8374777\n",
      "Loss at  733 39130.4358265\n",
      "Loss at  734 39130.0358911\n",
      "Loss at  735 39129.6376616\n",
      "Loss at  736 39129.241128\n",
      "Loss at  737 39128.8462805\n",
      "Loss at  738 39128.4531092\n",
      "Loss at  739 39128.0616045\n",
      "Loss at  740 39127.6717567\n",
      "Loss at  741 39127.2835561\n",
      "Loss at  742 39126.8969934\n",
      "Loss at  743 39126.512059\n",
      "Loss at  744 39126.1287436\n",
      "Loss at  745 39125.7470378\n",
      "Loss at  746 39125.3669325\n",
      "Loss at  747 39124.9884183\n",
      "Loss at  748 39124.6114863\n",
      "Loss at  749 39124.2361274\n",
      "Loss at  750 39123.8623326\n",
      "Loss at  751 39123.490093\n",
      "Loss at  752 39123.1193997\n",
      "Loss at  753 39122.7502439\n",
      "Loss at  754 39122.3826169\n",
      "Loss at  755 39122.01651\n",
      "Loss at  756 39121.6519147\n",
      "Loss at  757 39121.2888223\n",
      "Loss at  758 39120.9272244\n",
      "Loss at  759 39120.5671126\n",
      "Loss at  760 39120.2084784\n",
      "Loss at  761 39119.8513137\n",
      "Loss at  762 39119.4956101\n",
      "Loss at  763 39119.1413594\n",
      "Loss at  764 39118.7885535\n",
      "Loss at  765 39118.4371843\n",
      "Loss at  766 39118.0872439\n",
      "Loss at  767 39117.7387242\n",
      "Loss at  768 39117.3916173\n",
      "Loss at  769 39117.0459154\n",
      "Loss at  770 39116.7016106\n",
      "Loss at  771 39116.3586952\n",
      "Loss at  772 39116.0171616\n",
      "Loss at  773 39115.6770021\n",
      "Loss at  774 39115.338209\n",
      "Loss at  775 39115.0007749\n",
      "Loss at  776 39114.6646923\n",
      "Loss at  777 39114.3299537\n",
      "Loss at  778 39113.9965518\n",
      "Loss at  779 39113.6644792\n",
      "Loss at  780 39113.3337286\n",
      "Loss at  781 39113.0042928\n",
      "Loss at  782 39112.6761647\n",
      "Loss at  783 39112.3493371\n",
      "Loss at  784 39112.0238029\n",
      "Loss at  785 39111.6995551\n",
      "Loss at  786 39111.3765866\n",
      "Loss at  787 39111.0548906\n",
      "Loss at  788 39110.7344602\n",
      "Loss at  789 39110.4152884\n",
      "Loss at  790 39110.0973686\n",
      "Loss at  791 39109.7806939\n",
      "Loss at  792 39109.4652577\n",
      "Loss at  793 39109.1510532\n",
      "Loss at  794 39108.8380739\n",
      "Loss at  795 39108.5263132\n",
      "Loss at  796 39108.2157645\n",
      "Loss at  797 39107.9064214\n",
      "Loss at  798 39107.5982774\n",
      "Loss at  799 39107.2913261\n",
      "Loss at  800 39106.9855612\n",
      "Loss at  801 39106.6809763\n",
      "Loss at  802 39106.3775652\n",
      "Loss at  803 39106.0753216\n",
      "Loss at  804 39105.7742394\n",
      "Loss at  805 39105.4743124\n",
      "Loss at  806 39105.1755345\n",
      "Loss at  807 39104.8778996\n",
      "Loss at  808 39104.5814017\n",
      "Loss at  809 39104.2860348\n",
      "Loss at  810 39103.991793\n",
      "Loss at  811 39103.6986703\n",
      "Loss at  812 39103.4066609\n",
      "Loss at  813 39103.115759\n",
      "Loss at  814 39102.8259587\n",
      "Loss at  815 39102.5372543\n",
      "Loss at  816 39102.2496401\n",
      "Loss at  817 39101.9631104\n",
      "Loss at  818 39101.6776595\n",
      "Loss at  819 39101.3932819\n",
      "Loss at  820 39101.109972\n",
      "Loss at  821 39100.8277242\n",
      "Loss at  822 39100.5465331\n",
      "Loss at  823 39100.2663931\n",
      "Loss at  824 39099.9872989\n",
      "Loss at  825 39099.7092451\n",
      "Loss at  826 39099.4322263\n",
      "Loss at  827 39099.1562371\n",
      "Loss at  828 39098.8812724\n",
      "Loss at  829 39098.6073268\n",
      "Loss at  830 39098.3343952\n",
      "Loss at  831 39098.0624722\n",
      "Loss at  832 39097.7915529\n",
      "Loss at  833 39097.521632\n",
      "Loss at  834 39097.2527046\n",
      "Loss at  835 39096.9847654\n",
      "Loss at  836 39096.7178095\n",
      "Loss at  837 39096.451832\n",
      "Loss at  838 39096.1868278\n",
      "Loss at  839 39095.922792\n",
      "Loss at  840 39095.6597198\n",
      "Loss at  841 39095.3976062\n",
      "Loss at  842 39095.1364465\n",
      "Loss at  843 39094.8762358\n",
      "Loss at  844 39094.6169693\n",
      "Loss at  845 39094.3586424\n",
      "Loss at  846 39094.1012503\n",
      "Loss at  847 39093.8447883\n",
      "Loss at  848 39093.5892518\n",
      "Loss at  849 39093.3346361\n",
      "Loss at  850 39093.0809366\n",
      "Loss at  851 39092.8281489\n",
      "Loss at  852 39092.5762683\n",
      "Loss at  853 39092.3252903\n",
      "Loss at  854 39092.0752105\n",
      "Loss at  855 39091.8260244\n",
      "Loss at  856 39091.5777275\n",
      "Loss at  857 39091.3303156\n",
      "Loss at  858 39091.0837842\n",
      "Loss at  859 39090.8381289\n",
      "Loss at  860 39090.5933455\n",
      "Loss at  861 39090.3494297\n",
      "Loss at  862 39090.1063772\n",
      "Loss at  863 39089.8641837\n",
      "Loss at  864 39089.6228451\n",
      "Loss at  865 39089.3823572\n",
      "Loss at  866 39089.1427158\n",
      "Loss at  867 39088.9039168\n",
      "Loss at  868 39088.6659561\n",
      "Loss at  869 39088.4288296\n",
      "Loss at  870 39088.1925332\n",
      "Loss at  871 39087.9570629\n",
      "Loss at  872 39087.7224148\n",
      "Loss at  873 39087.4885847\n",
      "Loss at  874 39087.2555689\n",
      "Loss at  875 39087.0233632\n",
      "Loss at  876 39086.7919639\n",
      "Loss at  877 39086.5613671\n",
      "Loss at  878 39086.3315688\n",
      "Loss at  879 39086.1025652\n",
      "Loss at  880 39085.8743526\n",
      "Loss at  881 39085.6469271\n",
      "Loss at  882 39085.420285\n",
      "Loss at  883 39085.1944225\n",
      "Loss at  884 39084.9693359\n",
      "Loss at  885 39084.7450215\n",
      "Loss at  886 39084.5214756\n",
      "Loss at  887 39084.2986945\n",
      "Loss at  888 39084.0766747\n",
      "Loss at  889 39083.8554125\n",
      "Loss at  890 39083.6349043\n",
      "Loss at  891 39083.4151466\n",
      "Loss at  892 39083.1961358\n",
      "Loss at  893 39082.9778683\n",
      "Loss at  894 39082.7603407\n",
      "Loss at  895 39082.5435496\n",
      "Loss at  896 39082.3274913\n",
      "Loss at  897 39082.1121625\n",
      "Loss at  898 39081.8975598\n",
      "Loss at  899 39081.6836798\n",
      "Loss at  900 39081.470519\n",
      "Loss at  901 39081.2580741\n",
      "Loss at  902 39081.0463418\n",
      "Loss at  903 39080.8353188\n",
      "Loss at  904 39080.6250017\n",
      "Loss at  905 39080.4153873\n",
      "Loss at  906 39080.2064723\n",
      "Loss at  907 39079.9982534\n",
      "Loss at  908 39079.7907275\n",
      "Loss at  909 39079.5838913\n",
      "Loss at  910 39079.3777416\n",
      "Loss at  911 39079.1722754\n",
      "Loss at  912 39078.9674893\n",
      "Loss at  913 39078.7633803\n",
      "Loss at  914 39078.5599453\n",
      "Loss at  915 39078.3571812\n",
      "Loss at  916 39078.1550849\n",
      "Loss at  917 39077.9536534\n",
      "Loss at  918 39077.7528835\n",
      "Loss at  919 39077.5527724\n",
      "Loss at  920 39077.3533169\n",
      "Loss at  921 39077.1545141\n",
      "Loss at  922 39076.956361\n",
      "Loss at  923 39076.7588547\n",
      "Loss at  924 39076.5619923\n",
      "Loss at  925 39076.3657707\n",
      "Loss at  926 39076.1701871\n",
      "Loss at  927 39075.9752387\n",
      "Loss at  928 39075.7809225\n",
      "Loss at  929 39075.5872357\n",
      "Loss at  930 39075.3941755\n",
      "Loss at  931 39075.201739\n",
      "Loss at  932 39075.0099235\n",
      "Loss at  933 39074.8187261\n",
      "Loss at  934 39074.6281441\n",
      "Loss at  935 39074.4381747\n",
      "Loss at  936 39074.2488152\n",
      "Loss at  937 39074.0600629\n",
      "Loss at  938 39073.871915\n",
      "Loss at  939 39073.6843688\n",
      "Loss at  940 39073.4974218\n",
      "Loss at  941 39073.3110711\n",
      "Loss at  942 39073.1253143\n",
      "Loss at  943 39072.9401485\n",
      "Loss at  944 39072.7555713\n",
      "Loss at  945 39072.57158\n",
      "Loss at  946 39072.388172\n",
      "Loss at  947 39072.2053448\n",
      "Loss at  948 39072.0230958\n",
      "Loss at  949 39071.8414224\n",
      "Loss at  950 39071.6603222\n",
      "Loss at  951 39071.4797925\n",
      "Loss at  952 39071.2998309\n",
      "Loss at  953 39071.120435\n",
      "Loss at  954 39070.9416022\n",
      "Loss at  955 39070.7633301\n",
      "Loss at  956 39070.5856162\n",
      "Loss at  957 39070.4084581\n",
      "Loss at  958 39070.2318534\n",
      "Loss at  959 39070.0557996\n",
      "Loss at  960 39069.8802945\n",
      "Loss at  961 39069.7053355\n",
      "Loss at  962 39069.5309204\n",
      "Loss at  963 39069.3570468\n",
      "Loss at  964 39069.1837123\n",
      "Loss at  965 39069.0109147\n",
      "Loss at  966 39068.8386515\n",
      "Loss at  967 39068.6669206\n",
      "Loss at  968 39068.4957195\n",
      "Loss at  969 39068.3250461\n",
      "Loss at  970 39068.1548981\n",
      "Loss at  971 39067.9852732\n",
      "Loss at  972 39067.8161692\n",
      "Loss at  973 39067.6475839\n",
      "Loss at  974 39067.479515\n",
      "Loss at  975 39067.3119604\n",
      "Loss at  976 39067.1449178\n",
      "Loss at  977 39066.9783852\n",
      "Loss at  978 39066.8123602\n",
      "Loss at  979 39066.6468408\n",
      "Loss at  980 39066.4818248\n",
      "Loss at  981 39066.3173101\n",
      "Loss at  982 39066.1532946\n",
      "Loss at  983 39065.9897761\n",
      "Loss at  984 39065.8267526\n",
      "Loss at  985 39065.664222\n",
      "Loss at  986 39065.5021822\n",
      "Loss at  987 39065.3406312\n",
      "Loss at  988 39065.1795668\n",
      "Loss at  989 39065.0189872\n",
      "Loss at  990 39064.8588901\n",
      "Loss at  991 39064.6992737\n",
      "Loss at  992 39064.5401358\n",
      "Loss at  993 39064.3814746\n",
      "Loss at  994 39064.223288\n",
      "Loss at  995 39064.0655741\n",
      "Loss at  996 39063.9083308\n",
      "Loss at  997 39063.7515563\n",
      "Loss at  998 39063.5952485\n",
      "Loss at  999 39063.4394056\n"
     ]
    }
   ],
   "source": [
    "# optimize \n",
    "\n",
    "(R, Y) = load_movie_data()\n",
    "\n",
    "my_ratings = np.zeros((len(Y), 1))\n",
    "my_ratings[0, 0] = 4\n",
    "my_ratings[22, 0] = 3\n",
    "my_ratings[54, 0] = 4\n",
    "my_ratings[55, 0] = 5\n",
    "my_ratings[66, 0] = 3\n",
    "my_ratings[68, 0] = 5\n",
    "my_ratings[70, 0] = 4\n",
    "my_ratings[155, 0] = 4\n",
    "my_ratings[180, 0] = 4\n",
    "\n",
    "Y = np.hstack([my_ratings, Y])\n",
    "R = np.hstack([np.int32(my_ratings != 0), R])\n",
    "\n",
    "(Ynorm, Ymean) = normalize_ratings(Y, R)\n",
    "\n",
    "(n_movies, n_users) = Y.shape\n",
    "n_features = 10\n",
    "X = np.random.normal(size=(n_movies, n_features))\n",
    "Theta = np.random.normal(size=(n_users, n_features))\n",
    "\n",
    "n_iters = 1000\n",
    "alpha = 0.001\n",
    "l = 10\n",
    "for i in range(n_iters):\n",
    "    (loss, d_dx, d_dtheta) = J(X, R, Ynorm, Theta, l=l)\n",
    "    print('Loss at ', i, loss)\n",
    "    \n",
    "    X = X - (alpha * d_dx)\n",
    "    Theta = Theta - (alpha * d_dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 predictions\n",
      "7\n",
      "1673\n",
      "1674\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1680\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "p = X.dot(Theta.T)\n",
    "my_predictions = p[:, 0] + Ymean\n",
    "\n",
    "idx = np.argpartition(my_predictions, 10)\n",
    "\n",
    "print('Top 10 predictions')\n",
    "for i in idx[-10:]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
