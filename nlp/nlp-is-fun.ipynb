{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from https://medium.com/@ageitgey/natural-language-processing-is-fun-9a0bff37854e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"London is the capital and most populous city of England and \n",
    "the United Kingdom.  Standing on the River Thames in the south east \n",
    "of the island of Great Britain, London has been a major settlement \n",
    "for two millennia. It was founded by the Romans, who named it Londinium.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tip: download the package \"python -m spacy download en_core_web_sm\"\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"London\" (GPE)\n",
      "\"England\" (GPE)\n",
      "\"\n",
      "\" (GPE)\n",
      "\"the United Kingdom\" (GPE)\n",
      "\" \" (ORDINAL)\n",
      "\"the River Thames\" (ORG)\n",
      "\"\n",
      "\" (GPE)\n",
      "\"Great Britain\" (GPE)\n",
      "\"London\" (GPE)\n",
      "\"\n",
      "\" (GPE)\n",
      "\"two\" (CARDINAL)\n",
      "\"Romans\" (NORP)\n",
      "\"Londinium\" (PERSON)\n",
      "\"\n",
      "\" (GPE)\n"
     ]
    }
   ],
   "source": [
    "# pipeline (example):\n",
    "# * sentence segmentation\n",
    "# * tokenization\n",
    "# * parts of speech tagging\n",
    "# * lemmatization\n",
    "# * stop words\n",
    "# * dependency parsing\n",
    "# * named entity recognition\n",
    "# * coreference resolution\n",
    "\n",
    "doc = nlp(text) # runs the entire pipeline\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(f'\"{entity.text}\" ({entity.label_})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say that we need to remove all the names in several documents..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == 'PERSON':\n",
    "        return '[REDACTED]'\n",
    "\n",
    "    return token.string\n",
    "\n",
    "\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "        \n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return ''.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
      "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
      "\n",
      "---- ##### ----\n",
      "\n",
      "In 1950, [REDACTED]published his famous article \"Computing Machinery and Intelligence\". In 1957, [REDACTED]\n",
      "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery and Intelligence\". In 1957, Noam Chomsky’s \n",
    "Syntactic Structures revolutionized Linguistics with 'universal grammar', a rule based system of syntactic structures.\n",
    "\"\"\"\n",
    "\n",
    "print(s)\n",
    "print('---- ##### ----')\n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want some facts about a subject?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London\n",
      " - the capital and most populous city of England and  the United Kingdom.  \n",
      "\n",
      " - a major settlement  for two millennia.  \n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "statements = textacy.extract.semistructured_statements(doc, 'London')\n",
    "\n",
    "print('Here are the things I know about London')\n",
    "\n",
    "for statement in statements:\n",
    "    (subject, verb, fact) = statement\n",
    "    print(f' - {fact}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted some kind of autocompletion, like Google..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "major settlement\n",
      "united kingdom\n",
      "most populous city\n",
      "two millennia\n",
      "great britain\n",
      "south east\n",
      "river thames\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"London is the capital and most populous city of England and  the United Kingdom.  \n",
    "Standing on the River Thames in the south east of the island of Great Britain, \n",
    "London has been a major settlement  for two millennia.  It was founded by the Romans, \n",
    "who named it Londinium.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# if using a bigger text, increase min_freq\n",
    "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=1)\n",
    "\n",
    "noun_chunks = map(str, noun_chunks)\n",
    "noun_chunks = map(str.lower, noun_chunks)\n",
    "\n",
    "for noun_chunk in set(noun_chunks):\n",
    "    if len(noun_chunk.split(' ')) > 1:\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
