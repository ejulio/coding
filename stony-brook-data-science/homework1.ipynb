{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HOMEWORK 1\n",
    "\n",
    "* Work on New York Taxi Data (https://www.kaggle.com/c/new-york-city-taxi-fare-prediction/data)\n",
    "* https://docs.google.com/document/d/1T6YSc5Iy5Bg7giua8UjWgj222nLAwpKFg1ZLVtrFDVg/edit\n",
    "\n",
    "#### FROM THE SPEC ABOVE (Copied here for easy reference)\n",
    "1. Take a look at the training data. There may be anomalies in the data that you may need to factor in before you start on the other tasks. Clean the data first to handle these issues. Explain what you did to clean the data (in bulleted form). (10 pt)\n",
    "\n",
    "2. Compute the Pearson correlation between the following: (9 pt)\n",
    "    * Euclidean distance of the ride and the taxi fare\n",
    "    * time of day and distance traveled\n",
    "    * time of day and the taxi fare\n",
    "    * Which has the highest correlation?\n",
    "\n",
    "3. For each subtask of (2), create a plot visualizing the relation between the variables. Comment on whether you see non-linear or any other interesting relations. (9 pt)\n",
    "\n",
    "4. Create an exciting plot of your own using the dataset that you think reveals something very interesting.   Explain what it is, and anything else you learned. (15 pt)\n",
    "\n",
    "5. Generate additional features like those from (2) from the given data set. What additional features can you create? (10 pt)\n",
    "\n",
    "6. Set up a simple linear regression model to predict taxi fare. Use your generated features from the previous task if applicable. How well/badly does it work? What are the coefficients for your features? Which variable(s) are the most important one? (12 pt)\n",
    "\n",
    "7. Consider external datasets that may be helpful to expand your feature set. Give bullet points explaining all the datasets you could identify that would help improve your predictions. If possible, try finding such datasets online to incorporate into your training. List any that you were able to use in your analysis. (10 pt)\n",
    "\n",
    "8. Now, try to build a better prediction model that works harder to solve the task. Perhaps it will still use linear regression but with new features. Perhaps it will preprocess features better (e.g. normalize or scale the input vector, convert non-numerical value into float, or do a special treatment of missing values). Perhaps it will use a different machine learning approach (e.g. nearest neighbors, random forests, etc). Briefly explain what you did differently here versus the simple model. Which of your models minimizes the squared error? (10 pt)\n",
    "\n",
    "9. Predict all the taxi fares for instances at file “sample_submission.csv”. Write the result into a csv file and submit it to the website. You should do this for every model you develop. Report the rank, score, number of entries, for your highest rank. Include a snapshot of your best score on the leaderboard as confirmation. (15 pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some optimizations from https://www.kaggle.com/szelee/how-to-import-a-csv-file-of-55-million-rows\n",
    "# other approach would be to use dask http://docs.dask.org/en/latest/dataframe.html\n",
    "column_types = {'fare_amount': 'float32',\n",
    "              'pickup_datetime': 'str', \n",
    "              'pickup_longitude': 'float32',\n",
    "              'pickup_latitude': 'float32',\n",
    "              'dropoff_longitude': 'float32',\n",
    "              'dropoff_latitude': 'float32',\n",
    "              'passenger_count': 'uint8'}\n",
    "\n",
    "columns = list(column_types.keys())\n",
    "\n",
    "FILE_PATH = './homework1_data/train.csv'\n",
    "CHUNK_SIZE = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding/looking through the data\n",
    "\n",
    "Go through each chunk and identify anomalies.\n",
    "Examples: `passenger_count <= 0` or `fare_amount <= 0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_block(name, content):\n",
    "    print()\n",
    "    print(f'{\"-\" * 10} BEGIN: {name} {\"-\" * 10}')\n",
    "    print()\n",
    "    print(content)\n",
    "    print()\n",
    "    print(f'{\"-\" * 10} END: {name} {\"-\" * 10}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = pd.read_csv(FILE_PATH, usecols=columns, dtype=column_types, chunksize=CHUNK_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_size 10000\n",
      "fare_amount <= 0: 2\n",
      "passenger_count <= 0: 38\n",
      "\n",
      "---------- BEGIN: df.describe ----------\n",
      "\n",
      "        fare_amount  pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
      "count  10000.000000      10000.000000     10000.000000       10000.000000   \n",
      "mean      11.235464        -72.466660        39.920448         -72.474098   \n",
      "std        9.584258         10.609729         7.318932          10.579732   \n",
      "min       -2.900000        -74.438232       -74.006889         -74.429329   \n",
      "25%        6.000000        -73.992060        40.734546         -73.991112   \n",
      "50%        8.500000        -73.981758        40.752693         -73.980083   \n",
      "75%       12.500000        -73.966925        40.767694         -73.963505   \n",
      "max      180.000000         40.766125       401.083344          40.802437   \n",
      "\n",
      "       dropoff_latitude  passenger_count  \n",
      "count      10000.000000     10000.000000  \n",
      "mean          39.893280         1.644700  \n",
      "std            6.339919         1.271229  \n",
      "min          -73.994392         0.000000  \n",
      "25%           40.735230         1.000000  \n",
      "50%           40.753738         1.000000  \n",
      "75%           40.768186         2.000000  \n",
      "max           41.366138         6.000000  \n",
      "\n",
      "---------- END: df.describe ----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = next(chunks, None)\n",
    "if df is None:\n",
    "    print('No more data')\n",
    "else:\n",
    "    print('chunk_size', len(df))\n",
    "    print('fare_amount <= 0:', len(df.query('fare_amount <= 0')))\n",
    "    print('passenger_count <= 0:', len(df.query('passenger_count <= 0')))\n",
    "    print_block('df.describe', df.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
